<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Mongodb on Christian Kvalheim </title>
    <link>http://christiankvalheim.com/tags/mongodb/index.xml</link>
    <language>en-us</language>
    <author>Christian Kvalheim</author>
    <rights>Copyright (c) 2011 - 2014, Christian Kvalheim; all rights reserved.</rights>
    <updated>Thu, 01 May 2014 00:00:00 UTC</updated>
    
    <item>
      <title>Diagnosing driver installation problems</title>
      <link>http://christiankvalheim.com/post/diagnose_installation_problems</link>
      <pubDate>Thu, 01 May 2014 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/diagnose_installation_problems</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;The MongoDB driver is not installing correctly&lt;/h1&gt;

&lt;p&gt;Most of the problems are with the native BSON parser and there can be a multitude of reasons why it&amp;rsquo;s not installing correctly and the driver is falling back to the js implementation of the serializer. This post will hopefully help you track down what the problem is on your system. We will start looking at it from a checklist point of view.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Build Essentials Linux/Unix&lt;/h2&gt;

&lt;p&gt;If you don&amp;rsquo;t have the build essentials it won&amp;rsquo;t build. In the case of linux you will need gcc and g++, node.js with all the headers and python. The easiest way to figure out what&amp;rsquo;s missing is by trying to build the js-bson project. You can do this by performing the following steps.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/mongodb/js-bson.git
cd js-bson
npm install
make test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If all the steps complete you have the right toolchain installed. If you get node-gyp not found you need to install it globally by doing.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install -g node-gyp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If correctly compile and runs the tests you are golden. We can now try to install the mongod driver by performing the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd yourproject
npm install mongod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If it still fails the next step is to examine the npm log. Rerun the command but in this case in verbose mode.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm --loglevel verbose install mongodb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will print out all the steps npm is performing while trying to install the module.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Build Essentials Windows&lt;/h2&gt;

&lt;p&gt;In windows there are pre-compiled binaries for bson. &lt;strong&gt;However&lt;/strong&gt; these only work on 0.10.x or earlier as 0.11.x contains the completely breaking v8 API. So if you want to run 0.11.x you will have to set up your own build toolchain. So far I&amp;rsquo;ve only had luck with the following combination.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Visual Studio c++ 2010 (do not use higher versions)
Windows 7 64bit SDK
Python 2.7 or higher
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately you cannot easily use the make file under windows (I have not been able to do so correctly using MinGw).&lt;/p&gt;

&lt;p&gt;Open visual studio command prompt. Ensure node.exe is in your path and install node-gyp.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install -g node-gyp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next you will have to build the project manually to test it. Use any tool you use with git and grab the repo.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/mongodb/js-bson.git
cd js-bson
npm install
node-gyp rebuild
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should rebuild the driver successfully if you have everything set up correctly.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Other possible issues&lt;/h2&gt;

&lt;p&gt;Your python installation might be hosed making gyp break. I always recommend that you test your deployment environment first by trying to build node itself on the server in question as this should unearth any issues with broken packages (and there are a lot of broken packages out there).&lt;/p&gt;

&lt;p&gt;Another thing is to ensure your user has write permission to wherever the node modules are being installed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Diagnosing driver installation problems</title>
      <link>http://christiankvalheim.com/post/diagnose_installation_problems</link>
      <pubDate>Thu, 01 May 2014 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/diagnose_installation_problems</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;The MongoDB driver is not installing correctly&lt;/h1&gt;

&lt;p&gt;Most of the problems are with the native BSON parser and there can be a multitude of reasons why it&amp;rsquo;s not installing correctly and the driver is falling back to the js implementation of the serializer. This post will hopefully help you track down what the problem is on your system. We will start looking at it from a checklist point of view.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Build Essentials Linux/Unix&lt;/h2&gt;

&lt;p&gt;If you don&amp;rsquo;t have the build essentials it won&amp;rsquo;t build. In the case of linux you will need gcc and g++, node.js with all the headers and python. The easiest way to figure out what&amp;rsquo;s missing is by trying to build the js-bson project. You can do this by performing the following steps.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/mongodb/js-bson.git
cd js-bson
npm install
make test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If all the steps complete you have the right toolchain installed. If you get node-gyp not found you need to install it globally by doing.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install -g node-gyp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If correctly compile and runs the tests you are golden. We can now try to install the mongod driver by performing the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd yourproject
npm install mongod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If it still fails the next step is to examine the npm log. Rerun the command but in this case in verbose mode.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm --loglevel verbose install mongodb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will print out all the steps npm is performing while trying to install the module.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Build Essentials Windows&lt;/h2&gt;

&lt;p&gt;In windows there are pre-compiled binaries for bson. &lt;strong&gt;However&lt;/strong&gt; these only work on 0.10.x or earlier as 0.11.x contains the completely breaking v8 API. So if you want to run 0.11.x you will have to set up your own build toolchain. So far I&amp;rsquo;ve only had luck with the following combination.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Visual Studio c++ 2010 (do not use higher versions)
Windows 7 64bit SDK
Python 2.7 or higher
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately you cannot easily use the make file under windows (I have not been able to do so correctly using MinGw).&lt;/p&gt;

&lt;p&gt;Open visual studio command prompt. Ensure node.exe is in your path and install node-gyp.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install -g node-gyp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next you will have to build the project manually to test it. Use any tool you use with git and grab the repo.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/mongodb/js-bson.git
cd js-bson
npm install
node-gyp rebuild
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should rebuild the driver successfully if you have everything set up correctly.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Other possible issues&lt;/h2&gt;

&lt;p&gt;Your python installation might be hosed making gyp break. I always recommend that you test your deployment environment first by trying to build node itself on the server in question as this should unearth any issues with broken packages (and there are a lot of broken packages out there).&lt;/p&gt;

&lt;p&gt;Another thing is to ensure your user has write permission to wherever the node modules are being installed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The New Bulk API</title>
      <link>http://christiankvalheim.com/post/the_new_bulk_api</link>
      <pubDate>Wed, 16 Apr 2014 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/the_new_bulk_api</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;The New Bulk API&lt;/h1&gt;

&lt;p&gt;One of the core new features in MongoDB 2.6 is the new bulk write operations. All the drivers include a new bulk api that allows applications to leverage these new operations using a fluid style API. Let&amp;rsquo;s explore the API and how it&amp;rsquo;s implemented in the Node.js driver.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;The API&lt;/h2&gt;

&lt;p&gt;The API have two core concepts. The &lt;strong&gt;ordered&lt;/strong&gt; and the &lt;strong&gt;unordered&lt;/strong&gt; bulk operation. The main difference is in the way the operations in a bulk are executed. In the case of an &lt;strong&gt;ordered&lt;/strong&gt; bulk operation every operation will be executed in the order they are added to the bulk operation. In the case of an &lt;strong&gt;unordered&lt;/strong&gt; bulk operation however there is no guarantee what order the operations are executed. Later we will look at how they both are implemented.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Operations&lt;/h2&gt;

&lt;p&gt;You can initialize an &lt;strong&gt;ordered&lt;/strong&gt; or &lt;strong&gt;unordered&lt;/strong&gt; bulk operation in the following way.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ordered = db.collection(&#39;documents&#39;).initializeOrderedBulkOp();
var unordered = db.collection(&#39;documents&#39;).initializeUnorderedBulkOp();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you have a bulk operation you can start adding operations to the bulk. The following operations are valid.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;updateOne (update first matching document)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 1 }).updateOne({$inc : {x : 1}});
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;update (update all matching documents)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 1 }).update({$inc : {x : 2}});
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;replaceOne (replace entire document)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 1 }).replaceOne({ x : 2});
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_6&#34;&gt;updateOne or upsert (update first existing document or upsert)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 2 }).upsert().updateOne({ $inc : { x : 1}});
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_7&#34;&gt;update or upsert (update all or upsert)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 2 }).upsert().update({ $inc : { x : 2}});
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_8&#34;&gt;replace or upsert (replace first document or upsert)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 2 }).upsert().replaceOne({ x : 3 });
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_9&#34;&gt;removeOne (remove the first document matching)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 2 }).removeOne();
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_10&#34;&gt;remove (remove all documents matching)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 1 }).remove();
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_11&#34;&gt;insert&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.insert({ a : 5});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So what happens under the covers when you do start adding operations to a bulk operation. Let&amp;rsquo;s take a look at the new write operations.&lt;/p&gt;

&lt;h2 id=&#34;toc_12&#34;&gt;The New Write Operations&lt;/h2&gt;

&lt;p&gt;MongoDB 2.6 introduces a completely new set of write operations. Before 2.6 all write operations where done using wire protocol messages at the socket level. From 2.6 this changes to using commands. So what does these commands look like.&lt;/p&gt;

&lt;h3 id=&#34;toc_13&#34;&gt;Insert Write Command&lt;/h3&gt;

&lt;p&gt;The insert write commands allow an application insert batches of documents. Let&amp;rsquo;s take a look at the command and it&amp;rsquo;s options.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    insert: &#39;collection name&#39;
  , documents: [{ a : 1}, ...]
  , writeConcern: {
    w: 1, j: true, wtimeout: 1000
  }
  , ordered: true/false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A couple of things to note. The &lt;strong&gt;documents&lt;/strong&gt; field contains an array of all the documents that are to be inserted. The &lt;strong&gt;writeConcern&lt;/strong&gt; field specifies what would have previously been a &lt;strong&gt;getLastError&lt;/strong&gt; command that would follow the pre 2.6 write operations. In other words there is always a response from a write operation in 2.6. This means that &lt;strong&gt;w:0&lt;/strong&gt; has different semantics than what one is used to in pre 2.6. In the context &lt;strong&gt;w:0&lt;/strong&gt; basically means only return an &lt;strong&gt;ack&lt;/strong&gt; without any information about the &lt;strong&gt;success&lt;/strong&gt; or &lt;strong&gt;failure&lt;/strong&gt; of insert operations.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s quickly look at the update and remove write commands before we take a look at the results that are returned when executing these operations against 2.6.&lt;/p&gt;

&lt;h3 id=&#34;toc_14&#34;&gt;Update Write Command&lt;/h3&gt;

&lt;p&gt;There are some slight differences in the update write command in comparison to the insert write command. Let&amp;rsquo;s take a look.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    update: &#39;collection name&#39;
  , updates: [{ 
        q: { a : 1 }
      , u: { $inc : { x : 1}}
      , multi: true/false
      , upsert: true/false
    }, ...]
  , writeConcern: {
    w: 1, j: true, wtimeout: 1000
  }
  , ordered: true/false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We notice that the main difference here is that the updates array is an array of update operations where each entry in the array contains the &lt;strong&gt;q&lt;/strong&gt; field that specifies the selector for the update. The &lt;strong&gt;u&lt;/strong&gt; contains the update operation. &lt;strong&gt;multi&lt;/strong&gt; specifies if we will updateOne or updateAll documents that matches the selection. Finally &lt;strong&gt;upsert&lt;/strong&gt; tells the server if it will perform an upsert if the document is not found. Finally let&amp;rsquo;s look at the remove write command.&lt;/p&gt;

&lt;h3 id=&#34;toc_15&#34;&gt;Remove Write Command&lt;/h3&gt;

&lt;p&gt;The remove write command is very similar to the update write command. Let&amp;rsquo;s take a look at it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    delete: &#39;collection name&#39;
  , deletes: [{ 
        q: { a : 1 }
      , limit: 0/1
    }, ...]
  , writeConcern: {
    w: 1, j: true, wtimeout: 1000
  }
  , ordered: true/false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just an for updates we can see that the entries in the &lt;strong&gt;deletes&lt;/strong&gt; array contain documents with specific fields. The &lt;strong&gt;q&lt;/strong&gt; field is the selector that will match which documents will be removed. The &lt;strong&gt;limit&lt;/strong&gt; field sets the number of elements to be remove. Currently &lt;strong&gt;limit&lt;/strong&gt; only supports two values, 0 and 1. The value 0 for &lt;strong&gt;limit&lt;/strong&gt; removes all documents that match the selector. A value of 1 for &lt;strong&gt;limit&lt;/strong&gt; removes the first matching document only.&lt;/p&gt;

&lt;p&gt;So how does the response look when executing the commands against the server.&lt;/p&gt;

&lt;h3 id=&#34;toc_16&#34;&gt;Write Command Results&lt;/h3&gt;

&lt;p&gt;One of the best new aspects of the new write commands is that they can return information about each individual operation error in the batch. To avoid not having to transmit more information than necessary only information about errors are returned as well as the aggregated counts of successful operations. Let&amp;rsquo;s look at what a &lt;strong&gt;comprehensive&lt;/strong&gt;* result could look like.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;ok&amp;quot; : 1,
  &amp;quot;n&amp;quot; : 0,
  &amp;quot;nModified&amp;quot;: 1, (Applies only to update)
  &amp;quot;nRemoved&amp;quot;: 1, (Applies only to removes)
  &amp;quot;writeErrors&amp;quot; : [
    {
      &amp;quot;index&amp;quot; : 0,
      &amp;quot;code&amp;quot; : 11000,
      &amp;quot;errmsg&amp;quot; : &amp;quot;insertDocument :: caused by :: 11000 E11000 duplicate key error index: t1.t.$a_1  dup key: { : 1.0 }&amp;quot;
    }
  ],
  writeConcernError: {
    code : 22,
    errInfo: { wtimeout : true },
    errmsg: &amp;quot;Could not replicate operation within requested timeout&amp;quot;
  }      
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The two most interesting fields here are &lt;strong&gt;writeErrors&lt;/strong&gt; and &lt;strong&gt;writeConcernError&lt;/strong&gt;. If we take a look at &lt;strong&gt;writeErrors&lt;/strong&gt; we can see how it&amp;rsquo;s an array of objects that include an &lt;strong&gt;index&lt;/strong&gt; field as well as a &lt;strong&gt;code&lt;/strong&gt; and &lt;strong&gt;errmsg&lt;/strong&gt;. The &lt;strong&gt;field&lt;/strong&gt; references the position of the failing document in the original &lt;strong&gt;documents&lt;/strong&gt;, &lt;strong&gt;updates&lt;/strong&gt; or &lt;strong&gt;deletes&lt;/strong&gt; array allowing the application to identify the original batch document that failed.&lt;/p&gt;

&lt;h3 id=&#34;toc_17&#34;&gt;The Effect of Ordered (true/false)&lt;/h3&gt;

&lt;p&gt;The effect of setting &lt;strong&gt;ordered&lt;/strong&gt; to true or false have a direct implication on how a write command is processed. Most importantly if &lt;strong&gt;ordered&lt;/strong&gt; is set to &lt;strong&gt;true&lt;/strong&gt; the write operation will fail on the first write error (meaning the first error that fails to apply the operation to memory). If one sets &lt;strong&gt;ordered&lt;/strong&gt; to false however the operation will continue until all operations have been executed (potentially in parallel) and finally return all the results. &lt;strong&gt;writeConcernError&lt;/strong&gt; on the other hand does not stop the processing of a bulk operation as the document did not fail to be written to MongoDB, the writeConcern could not be applied.&lt;/p&gt;

&lt;p&gt;It helps to think of &lt;strong&gt;writeErrors&lt;/strong&gt; as &lt;strong&gt;hard&lt;/strong&gt; errors and &lt;strong&gt;writeConcernError&lt;/strong&gt; as a soft error.&lt;/p&gt;

&lt;h3 id=&#34;toc_18&#34;&gt;The Special Case of w:0&lt;/h3&gt;

&lt;p&gt;The semantics for &lt;strong&gt;w:0&lt;/strong&gt; changed for the write commands over the old style pre 2.6 write operations that are a combination of a write wire message and a &lt;strong&gt;getLastError&lt;/strong&gt; command afterwards. In the old style &lt;strong&gt;w:0&lt;/strong&gt; just meant that the driver would not send a &lt;strong&gt;getLastError&lt;/strong&gt; command after the write operation. However all write commands respond and thus the old semantics for &lt;strong&gt;w:0&lt;/strong&gt; are not possible to retain. The compromise is to make &lt;strong&gt;w:0&lt;/strong&gt; mean I don&amp;rsquo;t care about the results of the command just send me an &lt;strong&gt;Ack&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;So if you execute.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    insert: &#39;collection name&#39;
  , documents: [{ a : 1}, ...]
  , writeConcern: {
    w: 0
  }
  , ordered: true/false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All you receive from the server is the result&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ok : 1}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_19&#34;&gt;The Implication For The Bulk API&lt;/h2&gt;

&lt;p&gt;There are some implications to the fact that write commands are not mixed operations but either insert/update or removes. The Bulk API lets you mix operations and then merges the results back into a single result that simulates a mixed operations command in MongoDB. What does that mean in practice. Well let&amp;rsquo;s look at how node.js implements &lt;strong&gt;ordered&lt;/strong&gt; and &lt;strong&gt;unordered&lt;/strong&gt; bulk operations. Let&amp;rsquo;s use examples to show what happens.&lt;/p&gt;

&lt;h3 id=&#34;toc_20&#34;&gt;Ordered Operations&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s take the following set off operations performed on a bulk&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ordered = db.collection(&#39;documents&#39;).initializeOrderedBulkOp();
ordered.insert({ a : 1 });
ordered.find({ a : 1 }).update({ $inc: { x : 1 }});
ordered.insert({ a: 2 });
ordered.find({ a : 2 }).remove();
ordered.insert({ a: 3 });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When running in ordered mode the bulk API guarantees the ordering of the operations and thus will execute this as 5 operations one after the other.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;insert bulk operation
update bulk operation
insert bulk operation
remove bulk operation
insert bulk operation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have now reduced the bulk API to performing single operations and your throughput suffers accordingly.&lt;/p&gt;

&lt;p&gt;If we re-order our bulk operations in the following way.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ordered = db.collection(&#39;documents&#39;).initializeOrderedBulkOp();
ordered.insert({ a : 1 });
ordered.insert({ a: 2 });
ordered.insert({ a: 3 });
ordered.find({ a : 1 }).update({ $inc: { x : 1 }});
ordered.find({ a : 2 }).remove();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The execution is reduced to the following operations one after the other.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;insert bulk operation
update bulk operation
remove bulk operation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus for ordered bulk operations the ordered of operations will impact the number of write commands that need to be executed and thus the throughput possible.&lt;/p&gt;

&lt;h3 id=&#34;toc_21&#34;&gt;Unordered Operations&lt;/h3&gt;

&lt;p&gt;Unordered operations have not guarantee about the execution order of operations. Let&amp;rsquo;s take the operations example from above.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ordered = db.collection(&#39;documents&#39;).initializeOrderedBulkOp();
ordered.insert({ a : 1 });
ordered.find({ a : 1 }).update({ $inc: { x : 1 }});
ordered.insert({ a: 2 });
ordered.find({ a : 2 }).remove();
ordered.insert({ a: 3 });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Node.js driver will collect the operations into separate type specific operations. So we get.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;insert bulk operation
update bulk operation
remove bulk operation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In difference to the &lt;strong&gt;ordered&lt;/strong&gt; operation these bulks all get executed in parallel in Node.js and the results then merged when they have all finished.&lt;/p&gt;

&lt;h2 id=&#34;toc_22&#34;&gt;Takeaway&lt;/h2&gt;

&lt;p&gt;Due to MongoDb not currently implementing a mixed write operations command it&amp;rsquo;s important to keep this in mind when performing &lt;strong&gt;ordered&lt;/strong&gt; bulk operations to avoid the scenario above and take a hit on the write throughput. In the case of &lt;strong&gt;unordered&lt;/strong&gt; operations the missing mixed operations command does not impact the throughput.&lt;/p&gt;

&lt;p&gt;One thing to note. Although the Bulk API actually supports downconversion to 2.4 the performance impact is considerable as all operations are reduced to single write operations with a &lt;strong&gt;getLastError&lt;/strong&gt;. It&amp;rsquo;s recommended to leverage this API primarily with 2.6 or higher.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The New Bulk API</title>
      <link>http://christiankvalheim.com/post/the_new_bulk_api</link>
      <pubDate>Wed, 16 Apr 2014 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/the_new_bulk_api</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;The New Bulk API&lt;/h1&gt;

&lt;p&gt;One of the core new features in MongoDB 2.6 is the new bulk write operations. All the drivers include a new bulk api that allows applications to leverage these new operations using a fluid style API. Let&amp;rsquo;s explore the API and how it&amp;rsquo;s implemented in the Node.js driver.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;The API&lt;/h2&gt;

&lt;p&gt;The API have two core concepts. The &lt;strong&gt;ordered&lt;/strong&gt; and the &lt;strong&gt;unordered&lt;/strong&gt; bulk operation. The main difference is in the way the operations in a bulk are executed. In the case of an &lt;strong&gt;ordered&lt;/strong&gt; bulk operation every operation will be executed in the order they are added to the bulk operation. In the case of an &lt;strong&gt;unordered&lt;/strong&gt; bulk operation however there is no guarantee what order the operations are executed. Later we will look at how they both are implemented.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Operations&lt;/h2&gt;

&lt;p&gt;You can initialize an &lt;strong&gt;ordered&lt;/strong&gt; or &lt;strong&gt;unordered&lt;/strong&gt; bulk operation in the following way.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ordered = db.collection(&#39;documents&#39;).initializeOrderedBulkOp();
var unordered = db.collection(&#39;documents&#39;).initializeUnorderedBulkOp();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you have a bulk operation you can start adding operations to the bulk. The following operations are valid.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;updateOne (update first matching document)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 1 }).updateOne({$inc : {x : 1}});
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;update (update all matching documents)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 1 }).update({$inc : {x : 2}});
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;replaceOne (replace entire document)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 1 }).replaceOne({ x : 2});
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_6&#34;&gt;updateOne or upsert (update first existing document or upsert)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 2 }).upsert().updateOne({ $inc : { x : 1}});
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_7&#34;&gt;update or upsert (update all or upsert)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 2 }).upsert().update({ $inc : { x : 2}});
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_8&#34;&gt;replace or upsert (replace first document or upsert)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 2 }).upsert().replaceOne({ x : 3 });
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_9&#34;&gt;removeOne (remove the first document matching)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 2 }).removeOne();
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_10&#34;&gt;remove (remove all documents matching)&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.find({ a : 1 }).remove();
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_11&#34;&gt;insert&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ordered.insert({ a : 5});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So what happens under the covers when you do start adding operations to a bulk operation. Let&amp;rsquo;s take a look at the new write operations.&lt;/p&gt;

&lt;h2 id=&#34;toc_12&#34;&gt;The New Write Operations&lt;/h2&gt;

&lt;p&gt;MongoDB 2.6 introduces a completely new set of write operations. Before 2.6 all write operations where done using wire protocol messages at the socket level. From 2.6 this changes to using commands. So what does these commands look like.&lt;/p&gt;

&lt;h3 id=&#34;toc_13&#34;&gt;Insert Write Command&lt;/h3&gt;

&lt;p&gt;The insert write commands allow an application insert batches of documents. Let&amp;rsquo;s take a look at the command and it&amp;rsquo;s options.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    insert: &#39;collection name&#39;
  , documents: [{ a : 1}, ...]
  , writeConcern: {
    w: 1, j: true, wtimeout: 1000
  }
  , ordered: true/false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A couple of things to note. The &lt;strong&gt;documents&lt;/strong&gt; field contains an array of all the documents that are to be inserted. The &lt;strong&gt;writeConcern&lt;/strong&gt; field specifies what would have previously been a &lt;strong&gt;getLastError&lt;/strong&gt; command that would follow the pre 2.6 write operations. In other words there is always a response from a write operation in 2.6. This means that &lt;strong&gt;w:0&lt;/strong&gt; has different semantics than what one is used to in pre 2.6. In the context &lt;strong&gt;w:0&lt;/strong&gt; basically means only return an &lt;strong&gt;ack&lt;/strong&gt; without any information about the &lt;strong&gt;success&lt;/strong&gt; or &lt;strong&gt;failure&lt;/strong&gt; of insert operations.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s quickly look at the update and remove write commands before we take a look at the results that are returned when executing these operations against 2.6.&lt;/p&gt;

&lt;h3 id=&#34;toc_14&#34;&gt;Update Write Command&lt;/h3&gt;

&lt;p&gt;There are some slight differences in the update write command in comparison to the insert write command. Let&amp;rsquo;s take a look.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    update: &#39;collection name&#39;
  , updates: [{ 
        q: { a : 1 }
      , u: { $inc : { x : 1}}
      , multi: true/false
      , upsert: true/false
    }, ...]
  , writeConcern: {
    w: 1, j: true, wtimeout: 1000
  }
  , ordered: true/false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We notice that the main difference here is that the updates array is an array of update operations where each entry in the array contains the &lt;strong&gt;q&lt;/strong&gt; field that specifies the selector for the update. The &lt;strong&gt;u&lt;/strong&gt; contains the update operation. &lt;strong&gt;multi&lt;/strong&gt; specifies if we will updateOne or updateAll documents that matches the selection. Finally &lt;strong&gt;upsert&lt;/strong&gt; tells the server if it will perform an upsert if the document is not found. Finally let&amp;rsquo;s look at the remove write command.&lt;/p&gt;

&lt;h3 id=&#34;toc_15&#34;&gt;Remove Write Command&lt;/h3&gt;

&lt;p&gt;The remove write command is very similar to the update write command. Let&amp;rsquo;s take a look at it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    delete: &#39;collection name&#39;
  , deletes: [{ 
        q: { a : 1 }
      , limit: 0/1
    }, ...]
  , writeConcern: {
    w: 1, j: true, wtimeout: 1000
  }
  , ordered: true/false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just an for updates we can see that the entries in the &lt;strong&gt;deletes&lt;/strong&gt; array contain documents with specific fields. The &lt;strong&gt;q&lt;/strong&gt; field is the selector that will match which documents will be removed. The &lt;strong&gt;limit&lt;/strong&gt; field sets the number of elements to be remove. Currently &lt;strong&gt;limit&lt;/strong&gt; only supports two values, 0 and 1. The value 0 for &lt;strong&gt;limit&lt;/strong&gt; removes all documents that match the selector. A value of 1 for &lt;strong&gt;limit&lt;/strong&gt; removes the first matching document only.&lt;/p&gt;

&lt;p&gt;So how does the response look when executing the commands against the server.&lt;/p&gt;

&lt;h3 id=&#34;toc_16&#34;&gt;Write Command Results&lt;/h3&gt;

&lt;p&gt;One of the best new aspects of the new write commands is that they can return information about each individual operation error in the batch. To avoid not having to transmit more information than necessary only information about errors are returned as well as the aggregated counts of successful operations. Let&amp;rsquo;s look at what a &lt;strong&gt;comprehensive&lt;/strong&gt;* result could look like.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;ok&amp;quot; : 1,
  &amp;quot;n&amp;quot; : 0,
  &amp;quot;nModified&amp;quot;: 1, (Applies only to update)
  &amp;quot;nRemoved&amp;quot;: 1, (Applies only to removes)
  &amp;quot;writeErrors&amp;quot; : [
    {
      &amp;quot;index&amp;quot; : 0,
      &amp;quot;code&amp;quot; : 11000,
      &amp;quot;errmsg&amp;quot; : &amp;quot;insertDocument :: caused by :: 11000 E11000 duplicate key error index: t1.t.$a_1  dup key: { : 1.0 }&amp;quot;
    }
  ],
  writeConcernError: {
    code : 22,
    errInfo: { wtimeout : true },
    errmsg: &amp;quot;Could not replicate operation within requested timeout&amp;quot;
  }      
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The two most interesting fields here are &lt;strong&gt;writeErrors&lt;/strong&gt; and &lt;strong&gt;writeConcernError&lt;/strong&gt;. If we take a look at &lt;strong&gt;writeErrors&lt;/strong&gt; we can see how it&amp;rsquo;s an array of objects that include an &lt;strong&gt;index&lt;/strong&gt; field as well as a &lt;strong&gt;code&lt;/strong&gt; and &lt;strong&gt;errmsg&lt;/strong&gt;. The &lt;strong&gt;field&lt;/strong&gt; references the position of the failing document in the original &lt;strong&gt;documents&lt;/strong&gt;, &lt;strong&gt;updates&lt;/strong&gt; or &lt;strong&gt;deletes&lt;/strong&gt; array allowing the application to identify the original batch document that failed.&lt;/p&gt;

&lt;h3 id=&#34;toc_17&#34;&gt;The Effect of Ordered (true/false)&lt;/h3&gt;

&lt;p&gt;The effect of setting &lt;strong&gt;ordered&lt;/strong&gt; to true or false have a direct implication on how a write command is processed. Most importantly if &lt;strong&gt;ordered&lt;/strong&gt; is set to &lt;strong&gt;true&lt;/strong&gt; the write operation will fail on the first write error (meaning the first error that fails to apply the operation to memory). If one sets &lt;strong&gt;ordered&lt;/strong&gt; to false however the operation will continue until all operations have been executed (potentially in parallel) and finally return all the results. &lt;strong&gt;writeConcernError&lt;/strong&gt; on the other hand does not stop the processing of a bulk operation as the document did not fail to be written to MongoDB, the writeConcern could not be applied.&lt;/p&gt;

&lt;p&gt;It helps to think of &lt;strong&gt;writeErrors&lt;/strong&gt; as &lt;strong&gt;hard&lt;/strong&gt; errors and &lt;strong&gt;writeConcernError&lt;/strong&gt; as a soft error.&lt;/p&gt;

&lt;h3 id=&#34;toc_18&#34;&gt;The Special Case of w:0&lt;/h3&gt;

&lt;p&gt;The semantics for &lt;strong&gt;w:0&lt;/strong&gt; changed for the write commands over the old style pre 2.6 write operations that are a combination of a write wire message and a &lt;strong&gt;getLastError&lt;/strong&gt; command afterwards. In the old style &lt;strong&gt;w:0&lt;/strong&gt; just meant that the driver would not send a &lt;strong&gt;getLastError&lt;/strong&gt; command after the write operation. However all write commands respond and thus the old semantics for &lt;strong&gt;w:0&lt;/strong&gt; are not possible to retain. The compromise is to make &lt;strong&gt;w:0&lt;/strong&gt; mean I don&amp;rsquo;t care about the results of the command just send me an &lt;strong&gt;Ack&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;So if you execute.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    insert: &#39;collection name&#39;
  , documents: [{ a : 1}, ...]
  , writeConcern: {
    w: 0
  }
  , ordered: true/false
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All you receive from the server is the result&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ok : 1}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_19&#34;&gt;The Implication For The Bulk API&lt;/h2&gt;

&lt;p&gt;There are some implications to the fact that write commands are not mixed operations but either insert/update or removes. The Bulk API lets you mix operations and then merges the results back into a single result that simulates a mixed operations command in MongoDB. What does that mean in practice. Well let&amp;rsquo;s look at how node.js implements &lt;strong&gt;ordered&lt;/strong&gt; and &lt;strong&gt;unordered&lt;/strong&gt; bulk operations. Let&amp;rsquo;s use examples to show what happens.&lt;/p&gt;

&lt;h3 id=&#34;toc_20&#34;&gt;Ordered Operations&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s take the following set off operations performed on a bulk&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ordered = db.collection(&#39;documents&#39;).initializeOrderedBulkOp();
ordered.insert({ a : 1 });
ordered.find({ a : 1 }).update({ $inc: { x : 1 }});
ordered.insert({ a: 2 });
ordered.find({ a : 2 }).remove();
ordered.insert({ a: 3 });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When running in ordered mode the bulk API guarantees the ordering of the operations and thus will execute this as 5 operations one after the other.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;insert bulk operation
update bulk operation
insert bulk operation
remove bulk operation
insert bulk operation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have now reduced the bulk API to performing single operations and your throughput suffers accordingly.&lt;/p&gt;

&lt;p&gt;If we re-order our bulk operations in the following way.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ordered = db.collection(&#39;documents&#39;).initializeOrderedBulkOp();
ordered.insert({ a : 1 });
ordered.insert({ a: 2 });
ordered.insert({ a: 3 });
ordered.find({ a : 1 }).update({ $inc: { x : 1 }});
ordered.find({ a : 2 }).remove();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The execution is reduced to the following operations one after the other.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;insert bulk operation
update bulk operation
remove bulk operation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus for ordered bulk operations the ordered of operations will impact the number of write commands that need to be executed and thus the throughput possible.&lt;/p&gt;

&lt;h3 id=&#34;toc_21&#34;&gt;Unordered Operations&lt;/h3&gt;

&lt;p&gt;Unordered operations have not guarantee about the execution order of operations. Let&amp;rsquo;s take the operations example from above.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var ordered = db.collection(&#39;documents&#39;).initializeOrderedBulkOp();
ordered.insert({ a : 1 });
ordered.find({ a : 1 }).update({ $inc: { x : 1 }});
ordered.insert({ a: 2 });
ordered.find({ a : 2 }).remove();
ordered.insert({ a: 3 });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Node.js driver will collect the operations into separate type specific operations. So we get.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;insert bulk operation
update bulk operation
remove bulk operation
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In difference to the &lt;strong&gt;ordered&lt;/strong&gt; operation these bulks all get executed in parallel in Node.js and the results then merged when they have all finished.&lt;/p&gt;

&lt;h2 id=&#34;toc_22&#34;&gt;Takeaway&lt;/h2&gt;

&lt;p&gt;Due to MongoDb not currently implementing a mixed write operations command it&amp;rsquo;s important to keep this in mind when performing &lt;strong&gt;ordered&lt;/strong&gt; bulk operations to avoid the scenario above and take a hit on the write throughput. In the case of &lt;strong&gt;unordered&lt;/strong&gt; operations the missing mixed operations command does not impact the throughput.&lt;/p&gt;

&lt;p&gt;One thing to note. Although the Bulk API actually supports downconversion to 2.4 the performance impact is considerable as all operations are reduced to single write operations with a &lt;strong&gt;getLastError&lt;/strong&gt;. It&amp;rsquo;s recommended to leverage this API primarily with 2.6 or higher.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mongo Driver and Mongo DB 2.6 Features</title>
      <link>http://christiankvalheim.com/post/an_introduction_to_1_4_and_2_6</link>
      <pubDate>Sun, 16 Mar 2014 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/an_introduction_to_1_4_and_2_6</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;Mongo Driver and Mongo DB 2.6 Features&lt;/h1&gt;

&lt;p&gt;MongoDB 2.6 introduces some new powerful features that are reflected in the 1.4 driver release. These include.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Aggregation cursors&lt;/li&gt;
&lt;li&gt;Per query timeouts &lt;strong&gt;maxTimeMS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Ordered and Unordered bulk operations&lt;/li&gt;
&lt;li&gt;A parallelCollectionScan command for fast reading of an entire collection&lt;/li&gt;
&lt;li&gt;Integrated text search in the query language&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moreover the driver includes a whole slew of minor and major bug fixes and features. Some of the more noteworthy features include.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Better support for domains in node.js&lt;/li&gt;
&lt;li&gt;Reconnect events for replicaset and mongos connections&lt;/li&gt;
&lt;li&gt;Replicaset emits &amp;ldquo;joined&amp;rdquo; and &amp;ldquo;left&amp;rdquo; events when new server join or leave the set&lt;/li&gt;
&lt;li&gt;Added &lt;strong&gt;bufferMaxEntries&lt;/strong&gt; entry to allow tuning on how long driver keeps waiting for servers to come back up (default is until memory exhaustion)&lt;/li&gt;
&lt;li&gt;Upgraded BSON parser to rely on 0.2.6 returning to using &lt;strong&gt;nan&lt;/strong&gt; package&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s look at the main things in 2.6 features one by one.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Aggregation cursors&lt;/h2&gt;

&lt;p&gt;The addition off aggregation cursors to MongoDB 2.6 now means that applications can disregard the previous max result limit of 16MB. Let&amp;rsquo;s look at a simple use of the aggregation cursor.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get an aggregation cursor
  var cursor = db.collection(&#39;data&#39;).aggregate([
      {$match: {}}
    ], {
        allowDiskUsage: true
      , cursor: {batchSize: 1000}   
    });

  // Use cursor as stream
  cursor.on(&#39;data&#39;, function(data) {
    console.dir(data);
  });

  cursor.on(&#39;end&#39;, function() {
    db.close();
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As one can see the cursor implements the &lt;strong&gt;Readable&lt;/strong&gt; stream interface for 0.10.X or higher. For 2.4 the driver will emulate the cursor behavior by wrapping the result document.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;maxTimeMS&lt;/h2&gt;

&lt;p&gt;One feature that has requested often is the ability to timeout individual queries. In MongoDB 2.6 it&amp;rsquo;s finally arrived and is known as the &lt;strong&gt;maxTimeMS&lt;/strong&gt; option. Let&amp;rsquo;s take a look at a simple usage of the property with a query.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get an aggregation cursor
  var cursor = db.collection(&#39;data&#39;)
    .find(&amp;quot;$where&amp;quot;: &amp;quot;sleep(1000) || true&amp;quot;)
    .maxTimeMS(50);

  // Get alll the items
  cursor.toArray(function(err, items) {
    console.dir(err);
    console.dir(items);
    db.close();
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a bit of a contrived example using sleep to force the query to wait a second. With the &lt;strong&gt;maxTimeMS&lt;/strong&gt; set to 50 milliseconds the query will be aborted before the full second is up.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Ordered/Unordered bulk operations&lt;/h2&gt;

&lt;p&gt;Under the covers MongoDB is moving away from the combination of a write operation + get last error (GLE) and towards a write commands api. These new commands allow for the execution of bulk insert/update/remove operations. The bulk api&amp;rsquo;s are abstractions on top of this that server to make it easy to build bulk operations. Bulk operations come in two main flavors.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Ordered bulk operations. These operations execute all the operation in order and error out on the first write error.&lt;/li&gt;
&lt;li&gt;Unordered bulk operations. These operations execute all the operations in parallel and aggregates up all the errors. Unordered bulk operations do not guarantee order of execution.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s look at two simple examples using ordered and unordered operations.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get the collection
  var col = db.collection(&#39;batch_write_ordered_ops&#39;);
  // Initialize the Ordered Batch
  var batch = col.initializeOrderedBulkOp();

  // Add some operations to be executed in order
  batch.insert({a:1});
  batch.find({a:1}).updateOne({$set: {b:1}});
  batch.find({a:2}).upsert().updateOne({$set: {b:2}});
  batch.insert({a:3});
  batch.find({a:3}).remove({a:3});

  // Execute the operations
  batch.execute(function(err, result) {
    console.dir(err);
    console.dir(result);    
    db.close();
  });
});

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get the collection
  var col = db.collection(&#39;batch_write_ordered_ops&#39;);
  // Initialize the Ordered Batch
  var batch = col.initializeUnorderedBulkOp();

  // Add some operations to be executed in order
  batch.insert({a:1});
  batch.find({a:1}).updateOne({$set: {b:1}});
  batch.find({a:2}).upsert().updateOne({$set: {b:2}});
  batch.insert({a:3});
  batch.find({a:3}).remove({a:3});

  // Execute the operations
  batch.execute(function(err, result) {
    console.dir(err);
    console.dir(result);    
    db.close();
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For older servers than 2.6 the API will downconvert the operations. However it&amp;rsquo;s not possible to downconvert 100% so there might be slight edge cases where it cannot correctly report the right numbers.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;parallelCollectionScan&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;parallelCollectionScan&lt;/strong&gt; command is a special command targeted at reading out an entire collection using &lt;strong&gt;numCursors&lt;/strong&gt; parallel cursors.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get an aggregation cursor
  db.collection(&#39;data&#39;).parallelCollectionScan({numCursors:3}, function(err, cursors) {
    var results = [];

    for(var i = 0; i &amp;lt; cursors.length; i++) {
      cursors[i].get(function(err, items) {
        test.equal(err, null);

        // Add docs to results array
        results = results.concat(items);
        numCursors = numCursors - 1;

        // No more cursors let&#39;s ensure we got all results
        if(numCursors == 0) {
          test.equal(docs.length, results.length);

          db.close();
          test.done();
        }
      });
    }
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This optimizes the IO throughput from a collection.&lt;/p&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Integrated text search in the query language&lt;/h2&gt;

&lt;p&gt;Text indexes are now integrated into the main query language and enabled by default. A simple example.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get the collection
  var collection = db.collection(&#39;textSearchWithSort&#39;);
  collection.ensureIndex({s: &#39;text&#39;}, function(err, result) {
    test.equal(null, err);

    collection.insert([
        {s: &#39;spam&#39;}
      , {s: &#39;spam eggs and spam&#39;}
      , {s: &#39;sausage and eggs&#39;}], function(err, result) {
        test.equal(null, err);

        collection.find(
            {$text: {$search: &#39;spam&#39;}}
          , {fields: {_id: false, s: true, score: {$meta: &#39;textScore&#39;}}}
        ).sort({score: {$meta: &#39;textScore&#39;}}).toArray(function(err, items) {
          test.equal(null, err);
          test.equal(&amp;quot;spam eggs and spam&amp;quot;, items[0].s);
          db.close();
          test.done();
        });
      });
  });      
});
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Emitting Reconnect and Joined/Left events&lt;/h2&gt;

&lt;p&gt;The Replicaset and Mongos now emits events for servers joining and leaving the replicaset. This let&amp;rsquo;s applications more easily monitor the changes in the driver over time. &lt;strong&gt;Reconnect&lt;/strong&gt; in the context of a Replicaset or Mongos means that the driver is starting to replay buffered operations.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017,localhost:27027/test&amp;quot;, function(err, db) {
  db.serverConfig.on(&#39;joined&#39;, function(err, server) {
    console.log(&amp;quot;server joined&amp;quot;);
    console.dir(server);
  });

  db.serverConfig.on(&#39;left&#39;, function(err, server) {
    console.log(&amp;quot;server left&amp;quot;);
    console.dir(server);
  });

  db.serverConfig.on(&#39;reconnect&#39;, function() {
    console.log(&amp;quot;server reconnected&amp;quot;);
  }); 
});
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_7&#34;&gt;bufferMaxEntries&lt;/h2&gt;

&lt;p&gt;Buffered Max Entries allow for more fine grained control on how many operations that will be buffered before the driver errors out and stops attempting to reconnect.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, {
    db: {bufferMaxEntries:0},
  }, function(err, db) {
    db.close();
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example disables the command buffering completely and errors out the moment there is no connection available. The default value (for backward compatibility) is to buffer until memory runs out. Be aware that by setting a very low value you can cause some problems in failover scenarios in Replicasets as it might take a little but of time before f.ex a new Primary is elected and steps up to accept writes. Setting &lt;strong&gt;bufferMaxEntries&lt;/strong&gt; to 0 in this case will cause the driver to error out instead of falling over correctly.&lt;/p&gt;

&lt;h2 id=&#34;toc_8&#34;&gt;Fsync and journal Write Concerns note&lt;/h2&gt;

&lt;p&gt;MongoDB from version 2.6 and higher disallows the combination of &lt;strong&gt;journal&lt;/strong&gt; and &lt;strong&gt;fsync&lt;/strong&gt;. Combining them will cause an error while on 2.4 &lt;strong&gt;fsync&lt;/strong&gt; was ignored when provided with &lt;strong&gt;journal&lt;/strong&gt;. The following semantics apply.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;j: If true block until write operations have been committed to the journal. Cannot be used in combination with &lt;code&gt;fsync&lt;/code&gt;. Prior to MongoDB 2.6 this option was ignored if the server was running without journaling. Starting with MongoDB 2.6 write operations will fail with an exception if this option is used when the server is running without journaling.&lt;/li&gt;
&lt;li&gt;fsync: If true and the server is running without journaling, blocks until the server has synced all data files to disk. If the server is running with journaling, this acts the same as the &lt;code&gt;j&lt;/code&gt; option, blocking until write operations have been committed to the journal. Cannot be used in combination with &lt;code&gt;j&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Mongo Driver and Mongo DB 2.6 Features</title>
      <link>http://christiankvalheim.com/post/an_introduction_to_1_4_and_2_6</link>
      <pubDate>Sun, 16 Mar 2014 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/an_introduction_to_1_4_and_2_6</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;Mongo Driver and Mongo DB 2.6 Features&lt;/h1&gt;

&lt;p&gt;MongoDB 2.6 introduces some new powerful features that are reflected in the 1.4 driver release. These include.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Aggregation cursors&lt;/li&gt;
&lt;li&gt;Per query timeouts &lt;strong&gt;maxTimeMS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Ordered and Unordered bulk operations&lt;/li&gt;
&lt;li&gt;A parallelCollectionScan command for fast reading of an entire collection&lt;/li&gt;
&lt;li&gt;Integrated text search in the query language&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moreover the driver includes a whole slew of minor and major bug fixes and features. Some of the more noteworthy features include.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Better support for domains in node.js&lt;/li&gt;
&lt;li&gt;Reconnect events for replicaset and mongos connections&lt;/li&gt;
&lt;li&gt;Replicaset emits &amp;ldquo;joined&amp;rdquo; and &amp;ldquo;left&amp;rdquo; events when new server join or leave the set&lt;/li&gt;
&lt;li&gt;Added &lt;strong&gt;bufferMaxEntries&lt;/strong&gt; entry to allow tuning on how long driver keeps waiting for servers to come back up (default is until memory exhaustion)&lt;/li&gt;
&lt;li&gt;Upgraded BSON parser to rely on 0.2.6 returning to using &lt;strong&gt;nan&lt;/strong&gt; package&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s look at the main things in 2.6 features one by one.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Aggregation cursors&lt;/h2&gt;

&lt;p&gt;The addition off aggregation cursors to MongoDB 2.6 now means that applications can disregard the previous max result limit of 16MB. Let&amp;rsquo;s look at a simple use of the aggregation cursor.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get an aggregation cursor
  var cursor = db.collection(&#39;data&#39;).aggregate([
      {$match: {}}
    ], {
        allowDiskUsage: true
      , cursor: {batchSize: 1000}   
    });

  // Use cursor as stream
  cursor.on(&#39;data&#39;, function(data) {
    console.dir(data);
  });

  cursor.on(&#39;end&#39;, function() {
    db.close();
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As one can see the cursor implements the &lt;strong&gt;Readable&lt;/strong&gt; stream interface for 0.10.X or higher. For 2.4 the driver will emulate the cursor behavior by wrapping the result document.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;maxTimeMS&lt;/h2&gt;

&lt;p&gt;One feature that has requested often is the ability to timeout individual queries. In MongoDB 2.6 it&amp;rsquo;s finally arrived and is known as the &lt;strong&gt;maxTimeMS&lt;/strong&gt; option. Let&amp;rsquo;s take a look at a simple usage of the property with a query.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get an aggregation cursor
  var cursor = db.collection(&#39;data&#39;)
    .find(&amp;quot;$where&amp;quot;: &amp;quot;sleep(1000) || true&amp;quot;)
    .maxTimeMS(50);

  // Get alll the items
  cursor.toArray(function(err, items) {
    console.dir(err);
    console.dir(items);
    db.close();
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a bit of a contrived example using sleep to force the query to wait a second. With the &lt;strong&gt;maxTimeMS&lt;/strong&gt; set to 50 milliseconds the query will be aborted before the full second is up.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Ordered/Unordered bulk operations&lt;/h2&gt;

&lt;p&gt;Under the covers MongoDB is moving away from the combination of a write operation + get last error (GLE) and towards a write commands api. These new commands allow for the execution of bulk insert/update/remove operations. The bulk api&amp;rsquo;s are abstractions on top of this that server to make it easy to build bulk operations. Bulk operations come in two main flavors.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Ordered bulk operations. These operations execute all the operation in order and error out on the first write error.&lt;/li&gt;
&lt;li&gt;Unordered bulk operations. These operations execute all the operations in parallel and aggregates up all the errors. Unordered bulk operations do not guarantee order of execution.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s look at two simple examples using ordered and unordered operations.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get the collection
  var col = db.collection(&#39;batch_write_ordered_ops&#39;);
  // Initialize the Ordered Batch
  var batch = col.initializeOrderedBulkOp();

  // Add some operations to be executed in order
  batch.insert({a:1});
  batch.find({a:1}).updateOne({$set: {b:1}});
  batch.find({a:2}).upsert().updateOne({$set: {b:2}});
  batch.insert({a:3});
  batch.find({a:3}).remove({a:3});

  // Execute the operations
  batch.execute(function(err, result) {
    console.dir(err);
    console.dir(result);    
    db.close();
  });
});

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get the collection
  var col = db.collection(&#39;batch_write_ordered_ops&#39;);
  // Initialize the Ordered Batch
  var batch = col.initializeUnorderedBulkOp();

  // Add some operations to be executed in order
  batch.insert({a:1});
  batch.find({a:1}).updateOne({$set: {b:1}});
  batch.find({a:2}).upsert().updateOne({$set: {b:2}});
  batch.insert({a:3});
  batch.find({a:3}).remove({a:3});

  // Execute the operations
  batch.execute(function(err, result) {
    console.dir(err);
    console.dir(result);    
    db.close();
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For older servers than 2.6 the API will downconvert the operations. However it&amp;rsquo;s not possible to downconvert 100% so there might be slight edge cases where it cannot correctly report the right numbers.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;parallelCollectionScan&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;parallelCollectionScan&lt;/strong&gt; command is a special command targeted at reading out an entire collection using &lt;strong&gt;numCursors&lt;/strong&gt; parallel cursors.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get an aggregation cursor
  db.collection(&#39;data&#39;).parallelCollectionScan({numCursors:3}, function(err, cursors) {
    var results = [];

    for(var i = 0; i &amp;lt; cursors.length; i++) {
      cursors[i].get(function(err, items) {
        test.equal(err, null);

        // Add docs to results array
        results = results.concat(items);
        numCursors = numCursors - 1;

        // No more cursors let&#39;s ensure we got all results
        if(numCursors == 0) {
          test.equal(docs.length, results.length);

          db.close();
          test.done();
        }
      });
    }
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This optimizes the IO throughput from a collection.&lt;/p&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Integrated text search in the query language&lt;/h2&gt;

&lt;p&gt;Text indexes are now integrated into the main query language and enabled by default. A simple example.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get the collection
  var collection = db.collection(&#39;textSearchWithSort&#39;);
  collection.ensureIndex({s: &#39;text&#39;}, function(err, result) {
    test.equal(null, err);

    collection.insert([
        {s: &#39;spam&#39;}
      , {s: &#39;spam eggs and spam&#39;}
      , {s: &#39;sausage and eggs&#39;}], function(err, result) {
        test.equal(null, err);

        collection.find(
            {$text: {$search: &#39;spam&#39;}}
          , {fields: {_id: false, s: true, score: {$meta: &#39;textScore&#39;}}}
        ).sort({score: {$meta: &#39;textScore&#39;}}).toArray(function(err, items) {
          test.equal(null, err);
          test.equal(&amp;quot;spam eggs and spam&amp;quot;, items[0].s);
          db.close();
          test.done();
        });
      });
  });      
});
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Emitting Reconnect and Joined/Left events&lt;/h2&gt;

&lt;p&gt;The Replicaset and Mongos now emits events for servers joining and leaving the replicaset. This let&amp;rsquo;s applications more easily monitor the changes in the driver over time. &lt;strong&gt;Reconnect&lt;/strong&gt; in the context of a Replicaset or Mongos means that the driver is starting to replay buffered operations.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017,localhost:27027/test&amp;quot;, function(err, db) {
  db.serverConfig.on(&#39;joined&#39;, function(err, server) {
    console.log(&amp;quot;server joined&amp;quot;);
    console.dir(server);
  });

  db.serverConfig.on(&#39;left&#39;, function(err, server) {
    console.log(&amp;quot;server left&amp;quot;);
    console.dir(server);
  });

  db.serverConfig.on(&#39;reconnect&#39;, function() {
    console.log(&amp;quot;server reconnected&amp;quot;);
  }); 
});
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_7&#34;&gt;bufferMaxEntries&lt;/h2&gt;

&lt;p&gt;Buffered Max Entries allow for more fine grained control on how many operations that will be buffered before the driver errors out and stops attempting to reconnect.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, {
    db: {bufferMaxEntries:0},
  }, function(err, db) {
    db.close();
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example disables the command buffering completely and errors out the moment there is no connection available. The default value (for backward compatibility) is to buffer until memory runs out. Be aware that by setting a very low value you can cause some problems in failover scenarios in Replicasets as it might take a little but of time before f.ex a new Primary is elected and steps up to accept writes. Setting &lt;strong&gt;bufferMaxEntries&lt;/strong&gt; to 0 in this case will cause the driver to error out instead of falling over correctly.&lt;/p&gt;

&lt;h2 id=&#34;toc_8&#34;&gt;Fsync and journal Write Concerns note&lt;/h2&gt;

&lt;p&gt;MongoDB from version 2.6 and higher disallows the combination of &lt;strong&gt;journal&lt;/strong&gt; and &lt;strong&gt;fsync&lt;/strong&gt;. Combining them will cause an error while on 2.4 &lt;strong&gt;fsync&lt;/strong&gt; was ignored when provided with &lt;strong&gt;journal&lt;/strong&gt;. The following semantics apply.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;j: If true block until write operations have been committed to the journal. Cannot be used in combination with &lt;code&gt;fsync&lt;/code&gt;. Prior to MongoDB 2.6 this option was ignored if the server was running without journaling. Starting with MongoDB 2.6 write operations will fail with an exception if this option is used when the server is running without journaling.&lt;/li&gt;
&lt;li&gt;fsync: If true and the server is running without journaling, blocks until the server has synced all data files to disk. If the server is running with journaling, this acts the same as the &lt;code&gt;j&lt;/code&gt; option, blocking until write operations have been committed to the journal. Cannot be used in combination with &lt;code&gt;j&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lessons from 4 years of driver develoment</title>
      <link>http://christiankvalheim.com/presentation/four_years_of_node_and_mongodb</link>
      <pubDate>Tue, 01 Oct 2013 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/presentation/four_years_of_node_and_mongodb</guid>
      <description>&lt;figure &gt;
    
        &lt;img src=&#34;/media/jsconf2013.jpg&#34; alt=&#34;postgres con&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;p&gt;I gave the presentation Lessons from 4 years of driver develoment at JSConf EU 2013 in Berlin. The presentation is a summary of my four years of experience writing a successful open source project and all the good and bad experiences I&amp;rsquo;ve had over the last years in relation to this.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.slideshare.net/christkv/lessons-from-4-years-of-driver-develoment&#34;&gt;&lt;strong&gt;Lessons from 4 years of driver develoment&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&#34;embed slideshare&#34;&gt;
&lt;iframe src=&#34;http://www.slideshare.net/slideshow/embed_code/25310612?rel=0&#34; width=&#34;599&#34; height=&#34;487&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The video is also available&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=F9EQ8p5Jw9Q&#34;&gt;&lt;strong&gt;Lessons from 4 years of driver develoment&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;640&#34; height=&#34;385&#34; src=&#34;http://www.youtube.com/embed/F9EQ8p5Jw9Q&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How does mongodb store my data</title>
      <link>http://christiankvalheim.com/presentation/how_does_mongodb_store_my_data</link>
      <pubDate>Tue, 10 Apr 2012 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/presentation/how_does_mongodb_store_my_data</guid>
      <description>&lt;figure &gt;
    
        &lt;img src=&#34;/media/fosdem2013.png&#34; alt=&#34;postgres con&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;p&gt;At FOSDEM 2013 I gave a talk about MongoDB internals, going through how the database physically stores data in memory and disk and why you should learn about the internals of the database you use in production to let you get the maximum performance out of it and to be better able to diagnose issues in production.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.slideshare.net/christkv/storage-talk&#34;&gt;&lt;strong&gt;How does mongodb store my data&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&#34;embed slideshare&#34;&gt;
&lt;iframe src=&#34;http://www.slideshare.net/slideshow/embed_code/16327480?rel=0&#34; width=&#34;599&#34; height=&#34;487&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px&#34; allowfullscreen webkitallowfullscreen mozallowfullscreen&gt; &lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The wonderful world of GEO spatial indexes in MongoDB</title>
      <link>http://christiankvalheim.com/post/the_wonderful_world_of_geo_spatial_indexes_in_mongodb</link>
      <pubDate>Sun, 19 Feb 2012 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/the_wonderful_world_of_geo_spatial_indexes_in_mongodb</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;The wonderful world of GEO spatial indexes in MongoDB&lt;/h1&gt;

&lt;p&gt;MongoDB has native support for geospatial indexes and extensions to the query language to
support a lot of different ways of querying your geo spatial documents. We will touch on a
all of the available features of the MongoDB geospatial support point by point as outlined
below.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Query $near a point with a maximum distance around that point&lt;/li&gt;
&lt;li&gt;Set the minimum and maximum range for the 2d space letting you map any data to the space&lt;/li&gt;
&lt;li&gt;GeoNear command lets you return the distance from each point found&lt;/li&gt;
&lt;li&gt;$within query lets you set a shape for you query letting you use a circle, box or arbitrary polygon, letting you map complex geo queries such as congressional districts or post code zones.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But first let&amp;rsquo;s cover the basics of getting you up and running starting with what a document needs to look like
for the indexing to work.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Geospatialize your documents&lt;/h2&gt;

&lt;p&gt;Somehow we need to tell MongoDB what fields represent our geospatial coordinates. Luckily for us this is very simple. Lets take a simple sample document representing the best imaginative Burger place in the world.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var document = {
  name: &amp;quot;Awesome burger bar&amp;quot;      
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not we need know that it&amp;rsquo;s located on the fictitious planet (Burgoria) and more specifically at the coordinates
[50, 50]. So how do we add this to the document so we can look it up using geospatial searches ? Well it&amp;rsquo;s very
simple just add it as a field as shown below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var document = {
  name: &amp;quot;Awesome burger bar&amp;quot;,
  loc: [50, 50]      
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Easy right? The only thing you have to ensure is that the first coordinate is the &lt;strong&gt;x&lt;/strong&gt; coordinate and the second one is the &lt;strong&gt;y&lt;/strong&gt; coordinate &lt;strong&gt;[x, y]&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s go ahead and connect to the database and insert the document&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

var document = {
  name: &amp;quot;Awesome burger bar&amp;quot;,
  loc: [50, 50]      
}

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)

  db.collection(&#39;places&#39;).insert(document, {w:1}, function(err, result) {
    if(err) return console.dir(err)
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now we have a document in our collection. We now need to tell MongoDB to index our collection and create a 2D index on our loc attribute so we can avail us of the awesome geospatial features. This turns out to be easy as well. Let&amp;rsquo;s modify the code to ensure we have the index on startup.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

var document = {
  name: &amp;quot;Awesome burger bar&amp;quot;,
  loc: [50, 50]      
}

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)
  var collection = db.collection(&#39;places&#39;);

  collection.ensureIndex({loc: &amp;quot;2d&amp;quot;}, {min: -500, max: 500, w:1}, function(err, result) {
    if(err) return console.dir(err);

    collection.insert(document, {w:1}, function(err, result) {
      if(err) return console.dir(err)
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ensureIndex&lt;/strong&gt; does the trick creating the index if it does not already exist. By specifying &lt;strong&gt;{loc: &amp;ldquo;2d&amp;rdquo;}&lt;/strong&gt; MongoDB will index the array contained in every document under the field name &lt;strong&gt;loc&lt;/strong&gt;. The &lt;strong&gt;min&lt;/strong&gt; and &lt;strong&gt;max&lt;/strong&gt; defines the boundaries of our (Burgoria) and means that points outside -500 and 500 will throw an error as it&amp;rsquo;s not on the planet.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Basic queries for your geospatial documents&lt;/h2&gt;

&lt;p&gt;Since we now have a geospatial index on our collection let&amp;rsquo;s play around with the query methods and learn how we can work with the data. First however let&amp;rsquo;s add some more documents so we can see the effects of the different boundaries.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

var documents = [
    {name: &amp;quot;Awesome burger bar&amp;quot;, loc: [50, 50]}
  , {name: &amp;quot;Not an Awesome burger bar&amp;quot;, loc: [10, 10]}
  , {name: &amp;quot;More or less an Awesome burger bar&amp;quot;, loc: [45, 45]}
]

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)
  var collection = db.collection(&#39;places&#39;);

  collection.ensureIndex({loc: &amp;quot;2d&amp;quot;}, {min: -500, max: 500, w:1}, function(err, result) {
    if(err) return console.dir(err);

    collection.insert(documents, {w:1}, function(err, result) {
      if(err) return console.dir(err)
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right from now one for brevities sake we are going to assume we have the documents stored in the collection and the index created so we can work on queries without the boilerplate insert and index creation code. The first thing we are going to do is locate all the documents that&amp;rsquo;s a distance of 10 away from 50, 50.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  assert = require(&#39;assert&#39;);

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)

  db.collection(&#39;places&#39;).find({loc: {$near: [50,50], $maxDistance: 10}}).toArray(function(err, docs) {
    if(err) return console.dir(err)

    assert.equal(docs.length, 2);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This returns the following results (ignore the _id it will be different as it&amp;rsquo;s a collection assigned key).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee8e, &amp;quot;name&amp;quot; : &amp;quot;Awesome burger bar&amp;quot;, &amp;quot;loc&amp;quot; : [ 50, 50 ] }
{ &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee90, &amp;quot;name&amp;quot; : &amp;quot;More or less an Awesome burger bar&amp;quot;, &amp;quot;loc&amp;quot; : [ 45
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s look at the query. &lt;strong&gt;$near&lt;/strong&gt; specifies the center point for the geospatial query and &lt;strong&gt;$maxDistance&lt;/strong&gt; the radius of the search circle. Given this the query will return the two documents at &lt;strong&gt;[50, 50]&lt;/strong&gt; and &lt;strong&gt;[10, 10]&lt;/strong&gt;. Now this is a nice feature but what if we need to know the distance from each of the found documents to the originating center for our query. Luckily we have a command that support that called &lt;strong&gt;geoNear&lt;/strong&gt;. Let&amp;rsquo;s execute it and look at the results.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  assert = require(&#39;assert&#39;);

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)

  db.collection(&#39;places&#39;).geoNear(50, 50, {$maxDistance:10}, function(err, result) {
    if(err) return console.dir(err)

    assert.equal(result.results, 2);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s look at the results returned by the query.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;ns&amp;quot; : &amp;quot;test.places&amp;quot;,
  &amp;quot;near&amp;quot; : &amp;quot;1100000011110000111100001111000011110000111100001111&amp;quot;,
  &amp;quot;results&amp;quot; : [
    {
      &amp;quot;dis&amp;quot; : 0,
      &amp;quot;obj&amp;quot; : {
        &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee8e,
        &amp;quot;name&amp;quot; : &amp;quot;Awesome burger bar&amp;quot;,
        &amp;quot;loc&amp;quot; : [
          50,
          50
        ]
      }
    },
    {
      &amp;quot;dis&amp;quot; : 7.0710678118654755,
      &amp;quot;obj&amp;quot; : {
        &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee90,
        &amp;quot;name&amp;quot; : &amp;quot;More or less an Awesome burger bar&amp;quot;,
        &amp;quot;loc&amp;quot; : [
          45,
          45
        ]
      }
    }
  ],
  &amp;quot;stats&amp;quot; : {
    &amp;quot;time&amp;quot; : 0,
    &amp;quot;btreelocs&amp;quot; : 0,
    &amp;quot;nscanned&amp;quot; : 2,
    &amp;quot;objectsLoaded&amp;quot; : 2,
    &amp;quot;avgDistance&amp;quot; : 3.5355339059327378,
    &amp;quot;maxDistance&amp;quot; : 7.071128503792992
  },
  &amp;quot;ok&amp;quot; : 1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that &lt;strong&gt;geoNear&lt;/strong&gt; is a command not a find query so it returns a single document with the results in the results field of the returned document. As we can see from the results each returned result has a field called &lt;strong&gt;dis&lt;/strong&gt; that is the distance of the document from the center point of our search. Cool we&amp;rsquo;ve now covered the basics of geospatial search so let&amp;rsquo;s move onto more advanced queries.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Advanced queries for your geospatial documents&lt;/h2&gt;

&lt;p&gt;So besides these simple queries we can also do &lt;strong&gt;bounds queries&lt;/strong&gt;. With bounds queries we mean we can look for points of interest inside a defined boundary. This can be useful if you have such things as a post code area, congressional district or any sort of bounding box that is not a pure circle (say look for all restaurants in the west village in new york). Let&amp;rsquo;s go through the basics.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;The magical boundry box query&lt;/h3&gt;

&lt;p&gt;Our country Whopper on Burgoria is a perfectly bound box (imagine that). Our application wants to restrict our searches to only burger bars in Burgonia. The boundaries for Burgonia are defined by (30, 30) -&amp;gt; (30, 60) and (30, 60) -&amp;gt; (60, 60). Great let&amp;rsquo;s peform a box bounded query.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  assert = require(&#39;assert&#39;);

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)
  var box = [[30, 30], [60, 60]];

  db.collection(&#39;places&#39;).find({loc: {$within: {$box: box}}).toArray(function(err, docs) {
    if(err) return console.dir(err)

    assert.equal(docs.length, 2);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The results returned are.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee8e, &amp;quot;name&amp;quot; : &amp;quot;Awesome burger bar&amp;quot;, &amp;quot;loc&amp;quot; : [ 50, 50 ] }
{ &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee90, &amp;quot;name&amp;quot; : &amp;quot;More or less an Awesome burger bar&amp;quot;, &amp;quot;loc&amp;quot; : [ 45
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;A polygon to far&lt;/h3&gt;

&lt;p&gt;Awesome we can now do a query by our perfectly boxed country. Inside Whopper the country is split into triangles where triangle one is made up of three points (40, 40), (40, 50), (45, 45). We want to look for points that are only inside this triangle. Let&amp;rsquo;s have a look at the query.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  assert = require(&#39;assert&#39;);

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)
  var triangle = [[40, 40], [40, 50], [45, 45]];

  db.collection(&#39;places&#39;).find({loc: {$within: {$polygon: triangle}}).toArray(function(err, docs) {
    if(err) return console.dir(err)

    assert.equal(docs.length, 2);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The results returned are.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ &amp;quot;_id&amp;quot; : ObjectId(&amp;quot;509a47337d6ab61b2871ee90&amp;quot;), &amp;quot;name&amp;quot; : &amp;quot;More or less an Awesome burger bar&amp;quot;, &amp;quot;loc&amp;quot; : [ 45, 45 ] }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cool things you can use this with is f.ex with the data at &lt;a href=&#34;https://nycopendata.socrata.com/browse?tags=geographic&#34;&gt;https://nycopendata.socrata.com/browse?tags=geographic&lt;/a&gt; you can create queries slicing new york into areas and look for data points inside those areas. So we&amp;rsquo;ve seen how we can query geo spatially in a lot of different ways. In closing we want to mention some simple ideas to get your mind churning.&lt;/p&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Geospatial interesting tidbits&lt;/h2&gt;

&lt;p&gt;So geospatial is what we mostly promote the features as but at some point you&amp;rsquo;ll realize that it&amp;rsquo;s a generic set of 2d indexes that can be used to index and &lt;strong&gt;x,y&lt;/strong&gt; data. You could consider indexing any data points that fit into a 2d space and using the geo query functionality to retrieve subsets of that data. Say if you map price vs apartment size and want to say giving an apartment find me everything that is &amp;ldquo;close&amp;rdquo; to the ideal price and size that I&amp;rsquo;m looking for. The limit here is your fantasy but as you can see it&amp;rsquo;s a pretty general and very powerful feature once you get over looking at the feature as a pure geographical function. With that I leave you to experiment and have fun with the features we have introduced.&lt;/p&gt;

&lt;h2 id=&#34;toc_7&#34;&gt;Links and stuff&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/examples&#34;&gt;The driver examples, good starting point for basic usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/test&#34;&gt;All the integration tests, they have tons of different usage cases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mongodb.org/display/DOCS/Geospatial+Indexing&#34;&gt;MongoDB geospatial pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mongodb.org/display/DOCS/Geospatial+Haystack+Indexing&#34;&gt;More specialized geo haystack indexing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The wonderful world of GEO spatial indexes in MongoDB</title>
      <link>http://christiankvalheim.com/post/the_wonderful_world_of_geo_spatial_indexes_in_mongodb</link>
      <pubDate>Sun, 19 Feb 2012 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/the_wonderful_world_of_geo_spatial_indexes_in_mongodb</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;The wonderful world of GEO spatial indexes in MongoDB&lt;/h1&gt;

&lt;p&gt;MongoDB has native support for geospatial indexes and extensions to the query language to
support a lot of different ways of querying your geo spatial documents. We will touch on a
all of the available features of the MongoDB geospatial support point by point as outlined
below.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Query $near a point with a maximum distance around that point&lt;/li&gt;
&lt;li&gt;Set the minimum and maximum range for the 2d space letting you map any data to the space&lt;/li&gt;
&lt;li&gt;GeoNear command lets you return the distance from each point found&lt;/li&gt;
&lt;li&gt;$within query lets you set a shape for you query letting you use a circle, box or arbitrary polygon, letting you map complex geo queries such as congressional districts or post code zones.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But first let&amp;rsquo;s cover the basics of getting you up and running starting with what a document needs to look like
for the indexing to work.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Geospatialize your documents&lt;/h2&gt;

&lt;p&gt;Somehow we need to tell MongoDB what fields represent our geospatial coordinates. Luckily for us this is very simple. Lets take a simple sample document representing the best imaginative Burger place in the world.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var document = {
  name: &amp;quot;Awesome burger bar&amp;quot;      
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not we need know that it&amp;rsquo;s located on the fictitious planet (Burgoria) and more specifically at the coordinates
[50, 50]. So how do we add this to the document so we can look it up using geospatial searches ? Well it&amp;rsquo;s very
simple just add it as a field as shown below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var document = {
  name: &amp;quot;Awesome burger bar&amp;quot;,
  loc: [50, 50]      
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Easy right? The only thing you have to ensure is that the first coordinate is the &lt;strong&gt;x&lt;/strong&gt; coordinate and the second one is the &lt;strong&gt;y&lt;/strong&gt; coordinate &lt;strong&gt;[x, y]&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s go ahead and connect to the database and insert the document&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

var document = {
  name: &amp;quot;Awesome burger bar&amp;quot;,
  loc: [50, 50]      
}

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)

  db.collection(&#39;places&#39;).insert(document, {w:1}, function(err, result) {
    if(err) return console.dir(err)
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now we have a document in our collection. We now need to tell MongoDB to index our collection and create a 2D index on our loc attribute so we can avail us of the awesome geospatial features. This turns out to be easy as well. Let&amp;rsquo;s modify the code to ensure we have the index on startup.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

var document = {
  name: &amp;quot;Awesome burger bar&amp;quot;,
  loc: [50, 50]      
}

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)
  var collection = db.collection(&#39;places&#39;);

  collection.ensureIndex({loc: &amp;quot;2d&amp;quot;}, {min: -500, max: 500, w:1}, function(err, result) {
    if(err) return console.dir(err);

    collection.insert(document, {w:1}, function(err, result) {
      if(err) return console.dir(err)
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;ensureIndex&lt;/strong&gt; does the trick creating the index if it does not already exist. By specifying &lt;strong&gt;{loc: &amp;ldquo;2d&amp;rdquo;}&lt;/strong&gt; MongoDB will index the array contained in every document under the field name &lt;strong&gt;loc&lt;/strong&gt;. The &lt;strong&gt;min&lt;/strong&gt; and &lt;strong&gt;max&lt;/strong&gt; defines the boundaries of our (Burgoria) and means that points outside -500 and 500 will throw an error as it&amp;rsquo;s not on the planet.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Basic queries for your geospatial documents&lt;/h2&gt;

&lt;p&gt;Since we now have a geospatial index on our collection let&amp;rsquo;s play around with the query methods and learn how we can work with the data. First however let&amp;rsquo;s add some more documents so we can see the effects of the different boundaries.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

var documents = [
    {name: &amp;quot;Awesome burger bar&amp;quot;, loc: [50, 50]}
  , {name: &amp;quot;Not an Awesome burger bar&amp;quot;, loc: [10, 10]}
  , {name: &amp;quot;More or less an Awesome burger bar&amp;quot;, loc: [45, 45]}
]

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)
  var collection = db.collection(&#39;places&#39;);

  collection.ensureIndex({loc: &amp;quot;2d&amp;quot;}, {min: -500, max: 500, w:1}, function(err, result) {
    if(err) return console.dir(err);

    collection.insert(documents, {w:1}, function(err, result) {
      if(err) return console.dir(err)
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right from now one for brevities sake we are going to assume we have the documents stored in the collection and the index created so we can work on queries without the boilerplate insert and index creation code. The first thing we are going to do is locate all the documents that&amp;rsquo;s a distance of 10 away from 50, 50.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  assert = require(&#39;assert&#39;);

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)

  db.collection(&#39;places&#39;).find({loc: {$near: [50,50], $maxDistance: 10}}).toArray(function(err, docs) {
    if(err) return console.dir(err)

    assert.equal(docs.length, 2);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This returns the following results (ignore the _id it will be different as it&amp;rsquo;s a collection assigned key).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee8e, &amp;quot;name&amp;quot; : &amp;quot;Awesome burger bar&amp;quot;, &amp;quot;loc&amp;quot; : [ 50, 50 ] }
{ &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee90, &amp;quot;name&amp;quot; : &amp;quot;More or less an Awesome burger bar&amp;quot;, &amp;quot;loc&amp;quot; : [ 45
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s look at the query. &lt;strong&gt;$near&lt;/strong&gt; specifies the center point for the geospatial query and &lt;strong&gt;$maxDistance&lt;/strong&gt; the radius of the search circle. Given this the query will return the two documents at &lt;strong&gt;[50, 50]&lt;/strong&gt; and &lt;strong&gt;[10, 10]&lt;/strong&gt;. Now this is a nice feature but what if we need to know the distance from each of the found documents to the originating center for our query. Luckily we have a command that support that called &lt;strong&gt;geoNear&lt;/strong&gt;. Let&amp;rsquo;s execute it and look at the results.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  assert = require(&#39;assert&#39;);

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)

  db.collection(&#39;places&#39;).geoNear(50, 50, {$maxDistance:10}, function(err, result) {
    if(err) return console.dir(err)

    assert.equal(result.results, 2);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s look at the results returned by the query.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;ns&amp;quot; : &amp;quot;test.places&amp;quot;,
  &amp;quot;near&amp;quot; : &amp;quot;1100000011110000111100001111000011110000111100001111&amp;quot;,
  &amp;quot;results&amp;quot; : [
    {
      &amp;quot;dis&amp;quot; : 0,
      &amp;quot;obj&amp;quot; : {
        &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee8e,
        &amp;quot;name&amp;quot; : &amp;quot;Awesome burger bar&amp;quot;,
        &amp;quot;loc&amp;quot; : [
          50,
          50
        ]
      }
    },
    {
      &amp;quot;dis&amp;quot; : 7.0710678118654755,
      &amp;quot;obj&amp;quot; : {
        &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee90,
        &amp;quot;name&amp;quot; : &amp;quot;More or less an Awesome burger bar&amp;quot;,
        &amp;quot;loc&amp;quot; : [
          45,
          45
        ]
      }
    }
  ],
  &amp;quot;stats&amp;quot; : {
    &amp;quot;time&amp;quot; : 0,
    &amp;quot;btreelocs&amp;quot; : 0,
    &amp;quot;nscanned&amp;quot; : 2,
    &amp;quot;objectsLoaded&amp;quot; : 2,
    &amp;quot;avgDistance&amp;quot; : 3.5355339059327378,
    &amp;quot;maxDistance&amp;quot; : 7.071128503792992
  },
  &amp;quot;ok&amp;quot; : 1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that &lt;strong&gt;geoNear&lt;/strong&gt; is a command not a find query so it returns a single document with the results in the results field of the returned document. As we can see from the results each returned result has a field called &lt;strong&gt;dis&lt;/strong&gt; that is the distance of the document from the center point of our search. Cool we&amp;rsquo;ve now covered the basics of geospatial search so let&amp;rsquo;s move onto more advanced queries.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Advanced queries for your geospatial documents&lt;/h2&gt;

&lt;p&gt;So besides these simple queries we can also do &lt;strong&gt;bounds queries&lt;/strong&gt;. With bounds queries we mean we can look for points of interest inside a defined boundary. This can be useful if you have such things as a post code area, congressional district or any sort of bounding box that is not a pure circle (say look for all restaurants in the west village in new york). Let&amp;rsquo;s go through the basics.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;The magical boundry box query&lt;/h3&gt;

&lt;p&gt;Our country Whopper on Burgoria is a perfectly bound box (imagine that). Our application wants to restrict our searches to only burger bars in Burgonia. The boundaries for Burgonia are defined by (30, 30) -&amp;gt; (30, 60) and (30, 60) -&amp;gt; (60, 60). Great let&amp;rsquo;s peform a box bounded query.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  assert = require(&#39;assert&#39;);

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)
  var box = [[30, 30], [60, 60]];

  db.collection(&#39;places&#39;).find({loc: {$within: {$box: box}}).toArray(function(err, docs) {
    if(err) return console.dir(err)

    assert.equal(docs.length, 2);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The results returned are.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee8e, &amp;quot;name&amp;quot; : &amp;quot;Awesome burger bar&amp;quot;, &amp;quot;loc&amp;quot; : [ 50, 50 ] }
{ &amp;quot;_id&amp;quot; : 509a47337d6ab61b2871ee90, &amp;quot;name&amp;quot; : &amp;quot;More or less an Awesome burger bar&amp;quot;, &amp;quot;loc&amp;quot; : [ 45
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;A polygon to far&lt;/h3&gt;

&lt;p&gt;Awesome we can now do a query by our perfectly boxed country. Inside Whopper the country is split into triangles where triangle one is made up of three points (40, 40), (40, 50), (45, 45). We want to look for points that are only inside this triangle. Let&amp;rsquo;s have a look at the query.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  assert = require(&#39;assert&#39;);

MongoClient.connect(&amp;quot;mongodb://localhost:27017/geodb&amp;quot;, function(err, db) {
  if(err) return console.dir(err)
  var triangle = [[40, 40], [40, 50], [45, 45]];

  db.collection(&#39;places&#39;).find({loc: {$within: {$polygon: triangle}}).toArray(function(err, docs) {
    if(err) return console.dir(err)

    assert.equal(docs.length, 2);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The results returned are.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{ &amp;quot;_id&amp;quot; : ObjectId(&amp;quot;509a47337d6ab61b2871ee90&amp;quot;), &amp;quot;name&amp;quot; : &amp;quot;More or less an Awesome burger bar&amp;quot;, &amp;quot;loc&amp;quot; : [ 45, 45 ] }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cool things you can use this with is f.ex with the data at &lt;a href=&#34;https://nycopendata.socrata.com/browse?tags=geographic&#34;&gt;https://nycopendata.socrata.com/browse?tags=geographic&lt;/a&gt; you can create queries slicing new york into areas and look for data points inside those areas. So we&amp;rsquo;ve seen how we can query geo spatially in a lot of different ways. In closing we want to mention some simple ideas to get your mind churning.&lt;/p&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Geospatial interesting tidbits&lt;/h2&gt;

&lt;p&gt;So geospatial is what we mostly promote the features as but at some point you&amp;rsquo;ll realize that it&amp;rsquo;s a generic set of 2d indexes that can be used to index and &lt;strong&gt;x,y&lt;/strong&gt; data. You could consider indexing any data points that fit into a 2d space and using the geo query functionality to retrieve subsets of that data. Say if you map price vs apartment size and want to say giving an apartment find me everything that is &amp;ldquo;close&amp;rdquo; to the ideal price and size that I&amp;rsquo;m looking for. The limit here is your fantasy but as you can see it&amp;rsquo;s a pretty general and very powerful feature once you get over looking at the feature as a pure geographical function. With that I leave you to experiment and have fun with the features we have introduced.&lt;/p&gt;

&lt;h2 id=&#34;toc_7&#34;&gt;Links and stuff&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/examples&#34;&gt;The driver examples, good starting point for basic usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/test&#34;&gt;All the integration tests, they have tons of different usage cases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mongodb.org/display/DOCS/Geospatial+Indexing&#34;&gt;MongoDB geospatial pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mongodb.org/display/DOCS/Geospatial+Haystack+Indexing&#34;&gt;More specialized geo haystack indexing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A primer for GridFS using the Mongo DB driver</title>
      <link>http://christiankvalheim.com/post/a_primer_for_gridfs_using_the_mongodb_driver</link>
      <pubDate>Wed, 02 Mar 2011 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/a_primer_for_gridfs_using_the_mongodb_driver</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;A primer for GridFS using the Mongo DB driver&lt;/h1&gt;

&lt;p&gt;In the first tutorial we targeted general usage of the database. But Mongo DB is much more than this. One of the additional very useful features is to act as a file storage system. This is accomplish in Mongo by having a file collection and a chunks collection where each document in the chunks collection makes up a &lt;strong&gt;Block&lt;/strong&gt; of the file. In this tutorial we will look at how to use the GridFS functionality and what functions are available.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;A simple example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s dive straight into a simple example on how to write a file to the grid using the simplified Grid class.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  Grid = mongo.Grid;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) return console.dir(err);

  var grid = new Grid(db, &#39;fs&#39;);    
  var buffer = new Buffer(&amp;quot;Hello world&amp;quot;);
  grid.put(buffer, {metadata:{category:&#39;text&#39;}, content_type: &#39;text&#39;}, function(err, fileInfo) {
    if(!err) {
      console.log(&amp;quot;Finished writing file to Mongo&amp;quot;);
    }
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All right let&amp;rsquo;s dissect the example. The first thing you&amp;rsquo;ll notice is the statement&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var grid = new Grid(db, &#39;fs&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since GridFS is actually a special structure stored as collections you&amp;rsquo;ll notice that we are using the db connection that we used in the previous tutorial to operate on collections and documents. The second parameter &lt;strong&gt;&amp;lsquo;fs&amp;rsquo;&lt;/strong&gt; allows you to change the collections you want to store the data in. In this example the collections would be &lt;strong&gt;fs_files&lt;/strong&gt; and &lt;strong&gt;fs_chunks&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Having a live grid instance we now go ahead and create some test data stored in a Buffer instance, although you can pass in a string instead. We then write our data to disk.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var buffer = new Buffer(&amp;quot;Hello world&amp;quot;);
grid.put(buffer, {metadata:{category:&#39;text&#39;}, content_type: &#39;text&#39;}, function(err, fileInfo) {
  if(!err) {
    console.log(&amp;quot;Finished writing file to Mongo&amp;quot;);
  }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s deconstruct the call we just made. The &lt;strong&gt;put&lt;/strong&gt; call will write the data you passed in as one or more chunks. The second parameter is a hash of options for the Grid class. In this case we wish to annotate the file we are writing to Mongo DB with some metadata and also specify a content type. Each file entry in GridFS has support for metadata documents which might be very useful if you are for example storing images in you Mongo DB and need to store all the data associated with the image.&lt;/p&gt;

&lt;p&gt;One important thing is to take not that the put method return a document containing a &lt;strong&gt;_id&lt;/strong&gt;, this is an &lt;strong&gt;ObjectID&lt;/strong&gt; identifier that you&amp;rsquo;ll need to use if you wish to retrieve the file contents later.&lt;/p&gt;

&lt;p&gt;Right so we have written out first file, let&amp;rsquo;s look at the other two simple functions supported by the Grid class.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  Grid = mongo.Grid;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) return console.dir(err);

  var grid = new Grid(db, &#39;fs&#39;);    
  var buffer = new Buffer(&amp;quot;Hello world&amp;quot;);
  grid.put.(buffer, {metadata:{category:&#39;text&#39;}, content_type: &#39;text&#39;}, function(err, fileInfo) {        
    grid.get(fileInfo._id, function(err, data) {
      console.log(&amp;quot;Retrieved data: &amp;quot; + data.toString());
      grid.delete(fileInfo._id, function(err, result) {
      });        
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at the two operations &lt;strong&gt;get&lt;/strong&gt; and &lt;strong&gt;delete&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grid.get(fileInfo._id, function(err, data) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;get&lt;/strong&gt; method takes an ObjectID as the first argument and as we can se in the code we are using the one provided in &lt;strong&gt;fileInfo._id&lt;/strong&gt;. This will read all the chunks for the file and return it as a Buffer object.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;delete&lt;/strong&gt; method also takes an ObjectID as the first argument but will delete the file entry and the chunks associated with the file in Mongo.&lt;/p&gt;

&lt;p&gt;This &lt;strong&gt;api&lt;/strong&gt; is the simplest one you can use to interact with GridFS but it&amp;rsquo;s not suitable for all kinds of files. One of it&amp;rsquo;s main drawbacks is you are trying to write large files to Mongo. This api will require you to read the entire file into memory when writing and reading from Mongo which most likely is not feasible if you have to store large files like Video or RAW Pictures. Luckily this is not the only way to work with GridFS. That&amp;rsquo;s not to say this api is not useful. If you are storing tons of small files the memory usage vs the simplicity might be a worthwhile tradeoff. Let&amp;rsquo;s dive into some of the more advanced ways of using GridFS.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Advanced GridFS or how not to run out of memory&lt;/h2&gt;

&lt;p&gt;As we just said controlling memory consumption for you file writing and reading is key if you want to scale up the application. That means not reading in entire files before either writing or reading from Mongo DB. The good news is, it&amp;rsquo;s supported. Let&amp;rsquo;s throw some code out there straight away and look at how to do chunk sized streaming writes and reads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var fileId = new ObjectID();
var gridStore = new GridStore(db, fileId, &amp;quot;w&amp;quot;, {root:&#39;fs&#39;});
gridStore.chunkSize = 1024 * 256;

gridStore.open(function(err, gridStore) {
 Step(
   function writeData() {
     var group = this.group();

     for(var i = 0; i &amp;lt; 1000000; i += 5000) {
       gridStore.write(new Buffer(5000), group());
     }   
   },

   function doneWithWrite() {
     gridStore.close(function(err, result) {
       console.log(&amp;quot;File has been written to GridFS&amp;quot;);
     });
   }
 )
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we jump into picking apart the code let&amp;rsquo;s look at&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var gridStore = new GridStore(db, fileId, &amp;quot;w&amp;quot;, {root:&#39;fs&#39;});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the parameter &lt;strong&gt;&amp;ldquo;w&amp;rdquo;&lt;/strong&gt; this is important. It tells the driver that you are planning to write a new file. The parameters you can use here are.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;r&amp;rdquo;&lt;/strong&gt; - read only. This is the default mode&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;w&amp;rdquo;&lt;/strong&gt; - write in truncate mode. Existing data will be overwritten&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;w+&amp;rdquo;&lt;/strong&gt; - write in edit mode&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Right so there is a fair bit to digest here. We are simulating writing a file that&amp;rsquo;s about 1MB big to  Mongo DB using GridFS. To do this we are writing it in chunks of 5000 bytes. So to not live with a difficult callback setup we are using the Step library with its&amp;rsquo; group functionality to ensure that we are notified when all of the writes are done. After all the writes are done Step will invoke the next function (or step) called &lt;strong&gt;doneWithWrite&lt;/strong&gt; where we finish up by closing the file that flushes out any remaining data to Mongo DB and updates the file document.&lt;/p&gt;

&lt;p&gt;As we are doing it in chunks of 5000 bytes we will notice that memory consumption is low. This is the trick to write large files to GridFS. In pieces. Also notice this line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.chunkSize = 1024 * 256;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This allows you to adjust how big the chunks are in bytes that Mongo DB will write. You can tune the Chunk Size to your needs. If you need to write large files to GridFS it might be worthwhile to trade of memory for CPU by setting a larger Chunk Size.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s see how the actual streaming read works.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;new GridStore(db, fileId, &amp;quot;r&amp;quot;).open(function(err, gridStore) {
  var stream = gridStore.stream(true);

  stream.on(&amp;quot;data&amp;quot;, function(chunk) {
    console.log(&amp;quot;Chunk of file data&amp;quot;);
  });

  stream.on(&amp;quot;end&amp;quot;, function() {
    console.log(&amp;quot;EOF of file&amp;quot;);
  });

  stream.on(&amp;quot;close&amp;quot;, function() {
    console.log(&amp;quot;Finished reading the file&amp;quot;);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right let&amp;rsquo;s have a quick lock at the streaming functionality supplied with the driver &lt;strong&gt;(make sure you are using 0.9.6-12 or higher as there is a bug fix for custom chunksizes that you need)&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var stream = gridStore.stream(true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This opens a stream to our file, you can pass in a boolean parameter to tell the driver to close the file automatically when it reaches the end. This will fire the &lt;strong&gt;close&lt;/strong&gt; event automatically. Otherwise you&amp;rsquo;ll have to handle cleanup when you receive the &lt;strong&gt;end&lt;/strong&gt; event. Let&amp;rsquo;s have a look at the events supported.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  stream.on(&amp;quot;data&amp;quot;, function(chunk) {
    console.log(&amp;quot;Chunk of file data&amp;quot;);
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;data&lt;/strong&gt; event is called for each chunk read. This means that it&amp;rsquo;s by the chunk size of the written file. So if you file is 1MB big and the file has chunkSize 256K then you&amp;rsquo;ll get 4 calls to the event handler for &lt;strong&gt;data&lt;/strong&gt;. The chunk returned is a &lt;strong&gt;Buffer&lt;/strong&gt; object.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  stream.on(&amp;quot;end&amp;quot;, function() {
    console.log(&amp;quot;EOF of file&amp;quot;);
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;end&lt;/strong&gt; event is called when the driver reaches the end of data for the file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  stream.on(&amp;quot;close&amp;quot;, function() {
    console.log(&amp;quot;Finished reading the file&amp;quot;);
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;close&lt;/strong&gt; event is only called if you the &lt;strong&gt;autoclose&lt;/strong&gt; parameter on the &lt;strong&gt;gridStore.stream&lt;/strong&gt; method as shown above. If it&amp;rsquo;s false or not set handle cleanup of the streaming in the &lt;strong&gt;end&lt;/strong&gt; event handler.&lt;/p&gt;

&lt;p&gt;Right that&amp;rsquo;s it for writing to GridFS in an efficient Manner. I&amp;rsquo;ll outline some other useful function on the Gridstore object.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Other useful methods on the Gridstore object&lt;/h2&gt;

&lt;p&gt;There are some other methods that are useful&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.writeFile(filename/filedescriptor, function(err fileInfo) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;writeFile&lt;/strong&gt; takes either a file name or a file descriptor and writes it to GridFS. It does this in chunks to ensure the Eventloop is not tied up.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.read(length, function(err, data) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;read/readBuffer&lt;/strong&gt; lets you read a #length number of bytes from the current position in the file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.seek(position, seekLocation, function(err, gridStore) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;seek&lt;/strong&gt; lets you navigate the file to read from different positions inside the chunks. The seekLocation allows you to specify how to seek. It can be one of three values.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GridStore.IO_SEEK_SET Seek mode where the given length is absolute&lt;/li&gt;
&lt;li&gt;GridStore.IO_SEEK_CUR Seek mode where the given length is an offset to the current read/write head&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GridStore.IO_SEEK_END Seek mode where the given length is an offset to the end of the file&lt;/p&gt;

&lt;p&gt;GridStore.list(dbInstance, collectionName, {id:true}, function(err, files) {})&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;list&lt;/strong&gt; lists all the files in the collection in GridFS. If you have a lot of files the current version will not work very well as it&amp;rsquo;s getting all files into memory first. You can have it return either the filenames or the ids for the files using option.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.unlink(function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;unlink&lt;/strong&gt; deletes the file from Mongo DB, that&amp;rsquo;s to say all the file info and all the chunks.&lt;/p&gt;

&lt;p&gt;This should be plenty to get you on your way building your first GridFS based application. As in the previous article the following links might be useful for you. Good luck and have fun.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Links and stuff&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/examples&#34;&gt;The driver examples, good starting point for basic usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/test&#34;&gt;All the integration tests, they have tons of different usage cases&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A primer for GridFS using the Mongo DB driver</title>
      <link>http://christiankvalheim.com/post/a_primer_for_gridfs_using_the_mongodb_driver</link>
      <pubDate>Wed, 02 Mar 2011 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/a_primer_for_gridfs_using_the_mongodb_driver</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;A primer for GridFS using the Mongo DB driver&lt;/h1&gt;

&lt;p&gt;In the first tutorial we targeted general usage of the database. But Mongo DB is much more than this. One of the additional very useful features is to act as a file storage system. This is accomplish in Mongo by having a file collection and a chunks collection where each document in the chunks collection makes up a &lt;strong&gt;Block&lt;/strong&gt; of the file. In this tutorial we will look at how to use the GridFS functionality and what functions are available.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;A simple example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s dive straight into a simple example on how to write a file to the grid using the simplified Grid class.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  Grid = mongo.Grid;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) return console.dir(err);

  var grid = new Grid(db, &#39;fs&#39;);    
  var buffer = new Buffer(&amp;quot;Hello world&amp;quot;);
  grid.put(buffer, {metadata:{category:&#39;text&#39;}, content_type: &#39;text&#39;}, function(err, fileInfo) {
    if(!err) {
      console.log(&amp;quot;Finished writing file to Mongo&amp;quot;);
    }
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All right let&amp;rsquo;s dissect the example. The first thing you&amp;rsquo;ll notice is the statement&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var grid = new Grid(db, &#39;fs&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since GridFS is actually a special structure stored as collections you&amp;rsquo;ll notice that we are using the db connection that we used in the previous tutorial to operate on collections and documents. The second parameter &lt;strong&gt;&amp;lsquo;fs&amp;rsquo;&lt;/strong&gt; allows you to change the collections you want to store the data in. In this example the collections would be &lt;strong&gt;fs_files&lt;/strong&gt; and &lt;strong&gt;fs_chunks&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Having a live grid instance we now go ahead and create some test data stored in a Buffer instance, although you can pass in a string instead. We then write our data to disk.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var buffer = new Buffer(&amp;quot;Hello world&amp;quot;);
grid.put(buffer, {metadata:{category:&#39;text&#39;}, content_type: &#39;text&#39;}, function(err, fileInfo) {
  if(!err) {
    console.log(&amp;quot;Finished writing file to Mongo&amp;quot;);
  }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s deconstruct the call we just made. The &lt;strong&gt;put&lt;/strong&gt; call will write the data you passed in as one or more chunks. The second parameter is a hash of options for the Grid class. In this case we wish to annotate the file we are writing to Mongo DB with some metadata and also specify a content type. Each file entry in GridFS has support for metadata documents which might be very useful if you are for example storing images in you Mongo DB and need to store all the data associated with the image.&lt;/p&gt;

&lt;p&gt;One important thing is to take not that the put method return a document containing a &lt;strong&gt;_id&lt;/strong&gt;, this is an &lt;strong&gt;ObjectID&lt;/strong&gt; identifier that you&amp;rsquo;ll need to use if you wish to retrieve the file contents later.&lt;/p&gt;

&lt;p&gt;Right so we have written out first file, let&amp;rsquo;s look at the other two simple functions supported by the Grid class.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  Grid = mongo.Grid;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) return console.dir(err);

  var grid = new Grid(db, &#39;fs&#39;);    
  var buffer = new Buffer(&amp;quot;Hello world&amp;quot;);
  grid.put.(buffer, {metadata:{category:&#39;text&#39;}, content_type: &#39;text&#39;}, function(err, fileInfo) {        
    grid.get(fileInfo._id, function(err, data) {
      console.log(&amp;quot;Retrieved data: &amp;quot; + data.toString());
      grid.delete(fileInfo._id, function(err, result) {
      });        
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at the two operations &lt;strong&gt;get&lt;/strong&gt; and &lt;strong&gt;delete&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grid.get(fileInfo._id, function(err, data) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;get&lt;/strong&gt; method takes an ObjectID as the first argument and as we can se in the code we are using the one provided in &lt;strong&gt;fileInfo._id&lt;/strong&gt;. This will read all the chunks for the file and return it as a Buffer object.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;delete&lt;/strong&gt; method also takes an ObjectID as the first argument but will delete the file entry and the chunks associated with the file in Mongo.&lt;/p&gt;

&lt;p&gt;This &lt;strong&gt;api&lt;/strong&gt; is the simplest one you can use to interact with GridFS but it&amp;rsquo;s not suitable for all kinds of files. One of it&amp;rsquo;s main drawbacks is you are trying to write large files to Mongo. This api will require you to read the entire file into memory when writing and reading from Mongo which most likely is not feasible if you have to store large files like Video or RAW Pictures. Luckily this is not the only way to work with GridFS. That&amp;rsquo;s not to say this api is not useful. If you are storing tons of small files the memory usage vs the simplicity might be a worthwhile tradeoff. Let&amp;rsquo;s dive into some of the more advanced ways of using GridFS.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Advanced GridFS or how not to run out of memory&lt;/h2&gt;

&lt;p&gt;As we just said controlling memory consumption for you file writing and reading is key if you want to scale up the application. That means not reading in entire files before either writing or reading from Mongo DB. The good news is, it&amp;rsquo;s supported. Let&amp;rsquo;s throw some code out there straight away and look at how to do chunk sized streaming writes and reads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var fileId = new ObjectID();
var gridStore = new GridStore(db, fileId, &amp;quot;w&amp;quot;, {root:&#39;fs&#39;});
gridStore.chunkSize = 1024 * 256;

gridStore.open(function(err, gridStore) {
 Step(
   function writeData() {
     var group = this.group();

     for(var i = 0; i &amp;lt; 1000000; i += 5000) {
       gridStore.write(new Buffer(5000), group());
     }   
   },

   function doneWithWrite() {
     gridStore.close(function(err, result) {
       console.log(&amp;quot;File has been written to GridFS&amp;quot;);
     });
   }
 )
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we jump into picking apart the code let&amp;rsquo;s look at&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var gridStore = new GridStore(db, fileId, &amp;quot;w&amp;quot;, {root:&#39;fs&#39;});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the parameter &lt;strong&gt;&amp;ldquo;w&amp;rdquo;&lt;/strong&gt; this is important. It tells the driver that you are planning to write a new file. The parameters you can use here are.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;r&amp;rdquo;&lt;/strong&gt; - read only. This is the default mode&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;w&amp;rdquo;&lt;/strong&gt; - write in truncate mode. Existing data will be overwritten&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;w+&amp;rdquo;&lt;/strong&gt; - write in edit mode&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Right so there is a fair bit to digest here. We are simulating writing a file that&amp;rsquo;s about 1MB big to  Mongo DB using GridFS. To do this we are writing it in chunks of 5000 bytes. So to not live with a difficult callback setup we are using the Step library with its&amp;rsquo; group functionality to ensure that we are notified when all of the writes are done. After all the writes are done Step will invoke the next function (or step) called &lt;strong&gt;doneWithWrite&lt;/strong&gt; where we finish up by closing the file that flushes out any remaining data to Mongo DB and updates the file document.&lt;/p&gt;

&lt;p&gt;As we are doing it in chunks of 5000 bytes we will notice that memory consumption is low. This is the trick to write large files to GridFS. In pieces. Also notice this line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.chunkSize = 1024 * 256;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This allows you to adjust how big the chunks are in bytes that Mongo DB will write. You can tune the Chunk Size to your needs. If you need to write large files to GridFS it might be worthwhile to trade of memory for CPU by setting a larger Chunk Size.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s see how the actual streaming read works.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;new GridStore(db, fileId, &amp;quot;r&amp;quot;).open(function(err, gridStore) {
  var stream = gridStore.stream(true);

  stream.on(&amp;quot;data&amp;quot;, function(chunk) {
    console.log(&amp;quot;Chunk of file data&amp;quot;);
  });

  stream.on(&amp;quot;end&amp;quot;, function() {
    console.log(&amp;quot;EOF of file&amp;quot;);
  });

  stream.on(&amp;quot;close&amp;quot;, function() {
    console.log(&amp;quot;Finished reading the file&amp;quot;);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right let&amp;rsquo;s have a quick lock at the streaming functionality supplied with the driver &lt;strong&gt;(make sure you are using 0.9.6-12 or higher as there is a bug fix for custom chunksizes that you need)&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var stream = gridStore.stream(true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This opens a stream to our file, you can pass in a boolean parameter to tell the driver to close the file automatically when it reaches the end. This will fire the &lt;strong&gt;close&lt;/strong&gt; event automatically. Otherwise you&amp;rsquo;ll have to handle cleanup when you receive the &lt;strong&gt;end&lt;/strong&gt; event. Let&amp;rsquo;s have a look at the events supported.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  stream.on(&amp;quot;data&amp;quot;, function(chunk) {
    console.log(&amp;quot;Chunk of file data&amp;quot;);
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;data&lt;/strong&gt; event is called for each chunk read. This means that it&amp;rsquo;s by the chunk size of the written file. So if you file is 1MB big and the file has chunkSize 256K then you&amp;rsquo;ll get 4 calls to the event handler for &lt;strong&gt;data&lt;/strong&gt;. The chunk returned is a &lt;strong&gt;Buffer&lt;/strong&gt; object.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  stream.on(&amp;quot;end&amp;quot;, function() {
    console.log(&amp;quot;EOF of file&amp;quot;);
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;end&lt;/strong&gt; event is called when the driver reaches the end of data for the file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  stream.on(&amp;quot;close&amp;quot;, function() {
    console.log(&amp;quot;Finished reading the file&amp;quot;);
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;close&lt;/strong&gt; event is only called if you the &lt;strong&gt;autoclose&lt;/strong&gt; parameter on the &lt;strong&gt;gridStore.stream&lt;/strong&gt; method as shown above. If it&amp;rsquo;s false or not set handle cleanup of the streaming in the &lt;strong&gt;end&lt;/strong&gt; event handler.&lt;/p&gt;

&lt;p&gt;Right that&amp;rsquo;s it for writing to GridFS in an efficient Manner. I&amp;rsquo;ll outline some other useful function on the Gridstore object.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Other useful methods on the Gridstore object&lt;/h2&gt;

&lt;p&gt;There are some other methods that are useful&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.writeFile(filename/filedescriptor, function(err fileInfo) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;writeFile&lt;/strong&gt; takes either a file name or a file descriptor and writes it to GridFS. It does this in chunks to ensure the Eventloop is not tied up.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.read(length, function(err, data) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;read/readBuffer&lt;/strong&gt; lets you read a #length number of bytes from the current position in the file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.seek(position, seekLocation, function(err, gridStore) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;seek&lt;/strong&gt; lets you navigate the file to read from different positions inside the chunks. The seekLocation allows you to specify how to seek. It can be one of three values.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GridStore.IO_SEEK_SET Seek mode where the given length is absolute&lt;/li&gt;
&lt;li&gt;GridStore.IO_SEEK_CUR Seek mode where the given length is an offset to the current read/write head&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GridStore.IO_SEEK_END Seek mode where the given length is an offset to the end of the file&lt;/p&gt;

&lt;p&gt;GridStore.list(dbInstance, collectionName, {id:true}, function(err, files) {})&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;list&lt;/strong&gt; lists all the files in the collection in GridFS. If you have a lot of files the current version will not work very well as it&amp;rsquo;s getting all files into memory first. You can have it return either the filenames or the ids for the files using option.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.unlink(function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;unlink&lt;/strong&gt; deletes the file from Mongo DB, that&amp;rsquo;s to say all the file info and all the chunks.&lt;/p&gt;

&lt;p&gt;This should be plenty to get you on your way building your first GridFS based application. As in the previous article the following links might be useful for you. Good luck and have fun.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Links and stuff&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/examples&#34;&gt;The driver examples, good starting point for basic usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/test&#34;&gt;All the integration tests, they have tons of different usage cases&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Basic introduction to Mongo DB</title>
      <link>http://christiankvalheim.com/post/a_basic_introduction_to_mongodb</link>
      <pubDate>Mon, 28 Feb 2011 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/a_basic_introduction_to_mongodb</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;A Basic introduction to Mongo DB&lt;/h1&gt;

&lt;p&gt;Mongo DB has rapidly grown to become a popular database for web applications and is a perfect fit for Node.JS applications, letting you write Javascript for the client, backend and database layer. Its schemaless nature is a better match to our constantly evolving data structures in web applications, and the integrated support for location queries is a bonus that&amp;rsquo;s hard to ignore. Throw in Replica Sets for scaling, and we&amp;rsquo;re looking at really nice platform to grow your storage needs now and in the future.&lt;/p&gt;

&lt;p&gt;Now to shamelessly plug my driver. It can be downloaded via npm, or fetched from the github repository. To install via npm, do the following:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npm install mongodb&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;or go fetch it from github at &lt;a href=&#34;https://github.com/christkv/node-mongodb-native&#34;&gt;https://github.com/christkv/node-mongodb-native&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once this business is taken care of, let&amp;rsquo;s move through the types available for the driver and then how to connect to your Mongo DB instance before facing the usage of some CRUD operations.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Mongo DB data types&lt;/h2&gt;

&lt;p&gt;So there is an important thing to keep in mind when working with Mongo DB, and that is the slight mapping difference between types Mongo DB supports and native Javascript data types. Let&amp;rsquo;s have a look at the types supported out of the box and then how types are promoted by the driver to fit as close to native Javascript types as possible.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Float&lt;/strong&gt; is a 8 byte and is directly convertible to the Javascript type Number&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Double class&lt;/strong&gt; a special class representing a float value, this is especially useful when using capped collections where you need to ensure your values are always floats.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integers&lt;/strong&gt; is a bit trickier due to the fact that Javascript represents all Numbers as 64 bit floats meaning that the maximum integer value is at a 53 bit. Mongo has two types for integers, a 32 bit and a 64 bit. The driver will try to fit the value into 32 bits if it can and promote it to 64 bits if it has to. Similarly it will deserialize attempting to fit it into 53 bits if it can. If it cannot it will return an instance of &lt;strong&gt;Long&lt;/strong&gt; to avoid loosing precession.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Long class&lt;/strong&gt; a special class that let&amp;rsquo;s you store 64 bit integers and also let&amp;rsquo;s you operate on the 64 bits integers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Date&lt;/strong&gt; maps directly to a Javascript Date&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RegExp&lt;/strong&gt; maps directly to a Javascript RegExp&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;String&lt;/strong&gt; maps directly to a Javascript String (encoded in utf8)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Binary class&lt;/strong&gt; a special class that let&amp;rsquo;s you store data in Mongo DB&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Code class&lt;/strong&gt; a special class that let&amp;rsquo;s you store javascript functions in Mongo DB, can also provide a scope to run the method in&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ObjectID class&lt;/strong&gt; a special class that holds a MongoDB document identifier (the equivalent to a Primary key)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DbRef class&lt;/strong&gt; a special class that let&amp;rsquo;s you include a reference in a document pointing to another object&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Symbol class&lt;/strong&gt; a special class that let&amp;rsquo;s you specify a symbol, not really relevant for javascript but for languages that supports the concept of symbols.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As we see the number type can be a little tricky due to the way integers are implemented in Javascript. The latest driver will do correct conversion up to 53 bit&amp;rsquo;s of complexity. If you need to handle big integers the recommendation is to use the Long class to operate on the numbers.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Getting that connection to the database&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s get around to setting up a connection with the Mongo DB database. Jumping straight into the code let&amp;rsquo;s do direct connection and then look at the code.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(!err) {
    console.log(&amp;quot;We are connected&amp;quot;);
  }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s have a quick look at how the connection code works. The &lt;strong&gt;Db.connect&lt;/strong&gt;
method let&amp;rsquo;s use use a uri to connect to the Mongo database, where
&lt;strong&gt;localhost:27017&lt;/strong&gt; is the server host and port and &lt;strong&gt;exampleDb&lt;/strong&gt; the db
we wish to connect to. After the url notice the hash containing the
&lt;strong&gt;auto_reconnect&lt;/strong&gt; key. Auto reconnect tells the driver to retry sending
a command to the server if there is a failure during it&amp;rsquo;s execution.&lt;/p&gt;

&lt;p&gt;Another useful option you can pass in is&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;poolSize&lt;/strong&gt;, this allows you to control how many tcp connections are
opened in parallel. The default value for this is 5 but you can set it
as high as you want. The driver will use a round-robin strategy to
dispatch and read from the tcp connection.&lt;/p&gt;

&lt;p&gt;We are up and running with a connection to the database. Let&amp;rsquo;s move on
and look at what collections are and how they work.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Mongo DB and Collections&lt;/h2&gt;

&lt;p&gt;Collections are the equivalent of tables in traditional databases and contain all your documents. A database can have many collections. So how do we go about defining and using collections. Well there are a couple of methods that we can use. Let&amp;rsquo;s jump straight into code and then look at the code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) { return console.dir(err); }

  db.collection(&#39;test&#39;, function(err, collection) {});

  db.collection(&#39;test&#39;, {w:1}, function(err, collection) {});

  db.createCollection(&#39;test&#39;, function(err, collection) {});

  db.createCollection(&#39;test&#39;, {w:1}, function(err, collection) {});

});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Three different ways of creating a collection object but slightly different in behavior. Let&amp;rsquo;s go through them and see what they do&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.collection(&#39;test&#39;, function(err, collection) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function will not actually create a collection on the database until you actually insert the first document.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.collection(&#39;test&#39;, {strict:true}, function(err, collection) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the &lt;strong&gt;{strict:true}&lt;/strong&gt; option. This option will make the driver check if the collection exists and issue an error if it does not.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.createCollection(&#39;test&#39;, function(err, collection) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command will create the collection on the Mongo DB database before returning the collection object. If the collection already exists it will ignore the creation of the collection.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.createCollection(&#39;test&#39;, {strict:true}, function(err, collection) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;{strict:true}&lt;/strong&gt; option will make the method return an error if the collection already exists.&lt;/p&gt;

&lt;p&gt;With an open db connection and a collection defined we are ready to do some CRUD operation on the data.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;And then there was CRUD&lt;/h2&gt;

&lt;p&gt;So let&amp;rsquo;s get dirty with the basic operations for Mongo DB. The Mongo DB wire protocol is built around 4 main operations &lt;strong&gt;insert/update/remove/query&lt;/strong&gt;. Most operations on the database are actually queries with special json objects defining the operation on the database. But I&amp;rsquo;m getting ahead of myself. Let&amp;rsquo;s go back and look at insert first and do it with some code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) { return console.dir(err); }

  var collection = db.collection(&#39;test&#39;);
  var doc1 = {&#39;hello&#39;:&#39;doc1&#39;};
  var doc2 = {&#39;hello&#39;:&#39;doc2&#39;};
  var lotsOfDocs = [{&#39;hello&#39;:&#39;doc3&#39;}, {&#39;hello&#39;:&#39;doc4&#39;}];

  collection.insert(doc1);

  collection.insert(doc2, {w:1}, function(err, result) {});

  collection.insert(lotsOfDocs, {w:1}, function(err, result) {});

});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A couple of variations on the theme of inserting a document as we can see. To understand why it&amp;rsquo;s important to understand how Mongo DB works during inserts of documents.&lt;/p&gt;

&lt;p&gt;Mongo DB has asynchronous &lt;strong&gt;insert/update/remove&lt;/strong&gt; operations. This means that when you issue an &lt;strong&gt;insert&lt;/strong&gt; operation its a fire and forget operation where the database does not reply with the status of the insert operation. To retrieve the status of the operation you have to issue a query to retrieve the last error status of the connection. To make it simpler to the developer the driver implements the &lt;strong&gt;{w:1}&lt;/strong&gt; options so that this is done automatically when inserting the document. &lt;strong&gt;{w:1}&lt;/strong&gt; becomes especially important when you do &lt;strong&gt;update&lt;/strong&gt; or &lt;strong&gt;remove&lt;/strong&gt; as otherwise it&amp;rsquo;s not possible to determine the amount of documents modified or removed.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s go through the different types of inserts shown in the code above.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.insert(doc1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Taking advantage of the async behavior and not needing confirmation about the persisting of the data to Mongo DB we just fire off the insert (we are doing live analytics, loosing a couple of records does not matter).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.insert(doc2, {w:1}, function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That document needs to stick. Using the &lt;strong&gt;{w:1}&lt;/strong&gt; option ensure you get the error back if the document fails to insert correctly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.insert(lotsOfDocs, {w:1}, function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A batch insert of document with any errors being reported. This is much more efficient if you need to insert large batches of documents as you incur a lot less overhead.&lt;/p&gt;

&lt;p&gt;Right that&amp;rsquo;s the basics of insert&amp;rsquo;s ironed out. We got some documents in there but want to update them as we need to change the content of a field. Let&amp;rsquo;s have a look at a simple example and then we will dive into how Mongo DB updates work and how to do them efficiently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) { return console.dir(err); }

  var collection = db.collection(&#39;test&#39;);
  var doc = {mykey:1, fieldtoupdate:1};

  collection.insert(doc, {w:1}, function(err, result) {
    collection.update({mykey:1}, {$set:{fieldtoupdate:2}}, {w:1}, function(err, result) {});
  });

  var doc2 = {mykey:2, docs:[{doc1:1}]};

  collection.insert(doc2, {w:1}, function(err, result) {
    collection.update({mykey:2}, {$push:{docs:{doc2:1}}}, {w:1}, function(err, result) {});
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alright before we look at the code we want to understand how document updates work and how to do the efficiently. The most basic and less efficient way is to replace the whole document, this is not really the way to go if you want to change just a field in your document. Luckily Mongo DB provides a whole set of operations that let you modify just pieces of the document &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Atomic+Operations&#34;&gt;Atomic operations documentation&lt;/a&gt;. Basically outlined below.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$inc - increment a particular value by a certain amount&lt;/li&gt;
&lt;li&gt;$set - set a particular value&lt;/li&gt;
&lt;li&gt;$unset - delete a particular field (v1.3+)&lt;/li&gt;
&lt;li&gt;$push - append a value to an array&lt;/li&gt;
&lt;li&gt;$pushAll - append several values to an array&lt;/li&gt;
&lt;li&gt;$addToSet - adds value to the array only if its not in the array already&lt;/li&gt;
&lt;li&gt;$pop - removes the last element in an array&lt;/li&gt;
&lt;li&gt;$pull - remove a value(s) from an existing array&lt;/li&gt;
&lt;li&gt;$pullAll - remove several value(s) from an existing array&lt;/li&gt;
&lt;li&gt;$rename - renames the field&lt;/li&gt;
&lt;li&gt;$bit - bitwise operations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that the operations are outline let&amp;rsquo;s dig into the specific cases show in the code example.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.update({mykey:1}, {$set:{fieldtoupdate:2}}, {w:1}, function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right so this update will look for the document that has a field &lt;strong&gt;mykey&lt;/strong&gt; equal to &lt;strong&gt;1&lt;/strong&gt; and apply an update to the field &lt;strong&gt;fieldtoupdate&lt;/strong&gt; setting the value to &lt;strong&gt;2&lt;/strong&gt;. Since we are using the &lt;strong&gt;{w:1}&lt;/strong&gt; option the result parameter in the callback will return the value &lt;strong&gt;1&lt;/strong&gt; indicating that 1 document was modified by the update statement.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.update({mykey:2}, {$push:{docs:{doc2:1}}}, {w:1}, function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This updates adds another document to the field &lt;strong&gt;docs&lt;/strong&gt; in the document identified by &lt;strong&gt;{mykey:2}&lt;/strong&gt; using the atomic operation &lt;strong&gt;$push&lt;/strong&gt;. This allows you to modify keep such structures as queues in Mongo DB.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at the remove operation for the driver. As before let&amp;rsquo;s start with a piece of code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) { return console.dir(err); }

  var collection = db.collection(&#39;test&#39;);
  var docs = [{mykey:1}, {mykey:2}, {mykey:3}];

  collection.insert(docs, {w:1}, function(err, result) {

    collection.remove({mykey:1});

    collection.remove({mykey:2}, {w:1}, function(err, result) {});

    collection.remove();
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s examine the 3 remove variants and what they do.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.remove({mykey:1});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This leverages the fact that Mongo DB is asynchronous and that it does not return a result for &lt;strong&gt;insert/update/remove&lt;/strong&gt; to allow for &lt;strong&gt;synchronous&lt;/strong&gt; style execution. This particular remove query will remove the document where &lt;strong&gt;mykey&lt;/strong&gt; equals &lt;strong&gt;1&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.remove({mykey:2}, {w:1}, function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This remove statement removes the document where &lt;strong&gt;mykey&lt;/strong&gt; equals &lt;strong&gt;2&lt;/strong&gt; but since we are using &lt;strong&gt;{w:1}&lt;/strong&gt; it will back to Mongo DB to get the status of the remove operation and return the number of documents removed in the result variable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.remove();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This last one will remove all documents in the collection.&lt;/p&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Time to Query&lt;/h2&gt;

&lt;p&gt;Queries is of course a fundamental part of interacting with a database and Mongo DB is no exception. Fortunately for us it has a rich query interface with cursors and close to SQL concepts for slicing and dicing your datasets. To build queries we have lots of operators to choose from &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Advanced+Queries&#34;&gt;Mongo DB advanced queries&lt;/a&gt;. There are literarily tons of ways to search and ways to limit the query. Let&amp;rsquo;s look at some simple code for dealing with queries in different ways.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) { return console.dir(err); }

  var collection = db.collection(&#39;test&#39;);
  var docs = [{mykey:1}, {mykey:2}, {mykey:3}];

  collection.insert(docs, {w:1}, function(err, result) {

    collection.find().toArray(function(err, items) {});

    var stream = collection.find({mykey:{$ne:2}}).stream();
    stream.on(&amp;quot;data&amp;quot;, function(item) {});
    stream.on(&amp;quot;end&amp;quot;, function() {});

    collection.findOne({mykey:1}, function(err, item) {});

  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we start picking apart the code there is one thing that needs to be understood, the &lt;strong&gt;find&lt;/strong&gt; method does not execute the actual query. It builds an instance of &lt;strong&gt;Cursor&lt;/strong&gt; that you then use to retrieve the data. This lets you manage how you retrieve the data from Mongo DB and keeps state about your current Cursor state on Mongo DB. Now let&amp;rsquo;s pick apart the queries we have here and look at what they do.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.find().toArray(function(err, items) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This query will fetch all the document in the collection and return them as an array of items. Be careful with the function &lt;strong&gt;toArray&lt;/strong&gt; as it might cause a lot of memory usage as it will instantiate all the document into memory before returning the final array of items. If you have a big resultset you could run into memory issues.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var stream = collection.find({mykey:{$ne:2}}).stream();
stream.on(&amp;quot;data&amp;quot;, function(item) {});
stream.on(&amp;quot;end&amp;quot;, function() {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the preferred way if you have to retrieve a lot of data for streaming, as data is deserialized a &lt;strong&gt;data&lt;/strong&gt; event is emitted. This keeps the resident memory usage low as the documents are streamed to you. Very useful if you are pushing documents out via websockets or some other streaming socket protocol. Once there is no more document the driver will emit the &lt;strong&gt;end&lt;/strong&gt; event to notify the application that it&amp;rsquo;s done.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.findOne({mykey:1}, function(err, item) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is special supported function to retrieve just one specific document bypassing the need for a cursor object.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s pretty much it for the quick intro on how to use the database. I have also included a list of links to where to go to find more information and also a sample crude location application I wrote using express JS and mongo DB.&lt;/p&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Links and stuff&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/examples&#34;&gt;The driver examples, good starting point for basic usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/test&#34;&gt;All the integration tests, they have tons of different usage cases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mongodb.org/display/DOCS/Advanced+Queries&#34;&gt;The Mongo DB wiki pages such as the advanced query link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/christkv/mongodb-presentation&#34;&gt;A silly simple location based application using Express JS and Mongo DB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Basic introduction to Mongo DB</title>
      <link>http://christiankvalheim.com/post/a_basic_introduction_to_mongodb</link>
      <pubDate>Mon, 28 Feb 2011 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/a_basic_introduction_to_mongodb</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;A Basic introduction to Mongo DB&lt;/h1&gt;

&lt;p&gt;Mongo DB has rapidly grown to become a popular database for web applications and is a perfect fit for Node.JS applications, letting you write Javascript for the client, backend and database layer. Its schemaless nature is a better match to our constantly evolving data structures in web applications, and the integrated support for location queries is a bonus that&amp;rsquo;s hard to ignore. Throw in Replica Sets for scaling, and we&amp;rsquo;re looking at really nice platform to grow your storage needs now and in the future.&lt;/p&gt;

&lt;p&gt;Now to shamelessly plug my driver. It can be downloaded via npm, or fetched from the github repository. To install via npm, do the following:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;npm install mongodb&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;or go fetch it from github at &lt;a href=&#34;https://github.com/christkv/node-mongodb-native&#34;&gt;https://github.com/christkv/node-mongodb-native&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once this business is taken care of, let&amp;rsquo;s move through the types available for the driver and then how to connect to your Mongo DB instance before facing the usage of some CRUD operations.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Mongo DB data types&lt;/h2&gt;

&lt;p&gt;So there is an important thing to keep in mind when working with Mongo DB, and that is the slight mapping difference between types Mongo DB supports and native Javascript data types. Let&amp;rsquo;s have a look at the types supported out of the box and then how types are promoted by the driver to fit as close to native Javascript types as possible.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Float&lt;/strong&gt; is a 8 byte and is directly convertible to the Javascript type Number&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Double class&lt;/strong&gt; a special class representing a float value, this is especially useful when using capped collections where you need to ensure your values are always floats.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integers&lt;/strong&gt; is a bit trickier due to the fact that Javascript represents all Numbers as 64 bit floats meaning that the maximum integer value is at a 53 bit. Mongo has two types for integers, a 32 bit and a 64 bit. The driver will try to fit the value into 32 bits if it can and promote it to 64 bits if it has to. Similarly it will deserialize attempting to fit it into 53 bits if it can. If it cannot it will return an instance of &lt;strong&gt;Long&lt;/strong&gt; to avoid loosing precession.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Long class&lt;/strong&gt; a special class that let&amp;rsquo;s you store 64 bit integers and also let&amp;rsquo;s you operate on the 64 bits integers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Date&lt;/strong&gt; maps directly to a Javascript Date&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RegExp&lt;/strong&gt; maps directly to a Javascript RegExp&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;String&lt;/strong&gt; maps directly to a Javascript String (encoded in utf8)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Binary class&lt;/strong&gt; a special class that let&amp;rsquo;s you store data in Mongo DB&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Code class&lt;/strong&gt; a special class that let&amp;rsquo;s you store javascript functions in Mongo DB, can also provide a scope to run the method in&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ObjectID class&lt;/strong&gt; a special class that holds a MongoDB document identifier (the equivalent to a Primary key)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DbRef class&lt;/strong&gt; a special class that let&amp;rsquo;s you include a reference in a document pointing to another object&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Symbol class&lt;/strong&gt; a special class that let&amp;rsquo;s you specify a symbol, not really relevant for javascript but for languages that supports the concept of symbols.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As we see the number type can be a little tricky due to the way integers are implemented in Javascript. The latest driver will do correct conversion up to 53 bit&amp;rsquo;s of complexity. If you need to handle big integers the recommendation is to use the Long class to operate on the numbers.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Getting that connection to the database&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s get around to setting up a connection with the Mongo DB database. Jumping straight into the code let&amp;rsquo;s do direct connection and then look at the code.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(!err) {
    console.log(&amp;quot;We are connected&amp;quot;);
  }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s have a quick look at how the connection code works. The &lt;strong&gt;Db.connect&lt;/strong&gt;
method let&amp;rsquo;s use use a uri to connect to the Mongo database, where
&lt;strong&gt;localhost:27017&lt;/strong&gt; is the server host and port and &lt;strong&gt;exampleDb&lt;/strong&gt; the db
we wish to connect to. After the url notice the hash containing the
&lt;strong&gt;auto_reconnect&lt;/strong&gt; key. Auto reconnect tells the driver to retry sending
a command to the server if there is a failure during it&amp;rsquo;s execution.&lt;/p&gt;

&lt;p&gt;Another useful option you can pass in is&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;poolSize&lt;/strong&gt;, this allows you to control how many tcp connections are
opened in parallel. The default value for this is 5 but you can set it
as high as you want. The driver will use a round-robin strategy to
dispatch and read from the tcp connection.&lt;/p&gt;

&lt;p&gt;We are up and running with a connection to the database. Let&amp;rsquo;s move on
and look at what collections are and how they work.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Mongo DB and Collections&lt;/h2&gt;

&lt;p&gt;Collections are the equivalent of tables in traditional databases and contain all your documents. A database can have many collections. So how do we go about defining and using collections. Well there are a couple of methods that we can use. Let&amp;rsquo;s jump straight into code and then look at the code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) { return console.dir(err); }

  db.collection(&#39;test&#39;, function(err, collection) {});

  db.collection(&#39;test&#39;, {w:1}, function(err, collection) {});

  db.createCollection(&#39;test&#39;, function(err, collection) {});

  db.createCollection(&#39;test&#39;, {w:1}, function(err, collection) {});

});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Three different ways of creating a collection object but slightly different in behavior. Let&amp;rsquo;s go through them and see what they do&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.collection(&#39;test&#39;, function(err, collection) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function will not actually create a collection on the database until you actually insert the first document.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.collection(&#39;test&#39;, {strict:true}, function(err, collection) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the &lt;strong&gt;{strict:true}&lt;/strong&gt; option. This option will make the driver check if the collection exists and issue an error if it does not.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.createCollection(&#39;test&#39;, function(err, collection) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command will create the collection on the Mongo DB database before returning the collection object. If the collection already exists it will ignore the creation of the collection.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db.createCollection(&#39;test&#39;, {strict:true}, function(err, collection) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;{strict:true}&lt;/strong&gt; option will make the method return an error if the collection already exists.&lt;/p&gt;

&lt;p&gt;With an open db connection and a collection defined we are ready to do some CRUD operation on the data.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;And then there was CRUD&lt;/h2&gt;

&lt;p&gt;So let&amp;rsquo;s get dirty with the basic operations for Mongo DB. The Mongo DB wire protocol is built around 4 main operations &lt;strong&gt;insert/update/remove/query&lt;/strong&gt;. Most operations on the database are actually queries with special json objects defining the operation on the database. But I&amp;rsquo;m getting ahead of myself. Let&amp;rsquo;s go back and look at insert first and do it with some code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) { return console.dir(err); }

  var collection = db.collection(&#39;test&#39;);
  var doc1 = {&#39;hello&#39;:&#39;doc1&#39;};
  var doc2 = {&#39;hello&#39;:&#39;doc2&#39;};
  var lotsOfDocs = [{&#39;hello&#39;:&#39;doc3&#39;}, {&#39;hello&#39;:&#39;doc4&#39;}];

  collection.insert(doc1);

  collection.insert(doc2, {w:1}, function(err, result) {});

  collection.insert(lotsOfDocs, {w:1}, function(err, result) {});

});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A couple of variations on the theme of inserting a document as we can see. To understand why it&amp;rsquo;s important to understand how Mongo DB works during inserts of documents.&lt;/p&gt;

&lt;p&gt;Mongo DB has asynchronous &lt;strong&gt;insert/update/remove&lt;/strong&gt; operations. This means that when you issue an &lt;strong&gt;insert&lt;/strong&gt; operation its a fire and forget operation where the database does not reply with the status of the insert operation. To retrieve the status of the operation you have to issue a query to retrieve the last error status of the connection. To make it simpler to the developer the driver implements the &lt;strong&gt;{w:1}&lt;/strong&gt; options so that this is done automatically when inserting the document. &lt;strong&gt;{w:1}&lt;/strong&gt; becomes especially important when you do &lt;strong&gt;update&lt;/strong&gt; or &lt;strong&gt;remove&lt;/strong&gt; as otherwise it&amp;rsquo;s not possible to determine the amount of documents modified or removed.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s go through the different types of inserts shown in the code above.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.insert(doc1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Taking advantage of the async behavior and not needing confirmation about the persisting of the data to Mongo DB we just fire off the insert (we are doing live analytics, loosing a couple of records does not matter).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.insert(doc2, {w:1}, function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That document needs to stick. Using the &lt;strong&gt;{w:1}&lt;/strong&gt; option ensure you get the error back if the document fails to insert correctly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.insert(lotsOfDocs, {w:1}, function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A batch insert of document with any errors being reported. This is much more efficient if you need to insert large batches of documents as you incur a lot less overhead.&lt;/p&gt;

&lt;p&gt;Right that&amp;rsquo;s the basics of insert&amp;rsquo;s ironed out. We got some documents in there but want to update them as we need to change the content of a field. Let&amp;rsquo;s have a look at a simple example and then we will dive into how Mongo DB updates work and how to do them efficiently.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) { return console.dir(err); }

  var collection = db.collection(&#39;test&#39;);
  var doc = {mykey:1, fieldtoupdate:1};

  collection.insert(doc, {w:1}, function(err, result) {
    collection.update({mykey:1}, {$set:{fieldtoupdate:2}}, {w:1}, function(err, result) {});
  });

  var doc2 = {mykey:2, docs:[{doc1:1}]};

  collection.insert(doc2, {w:1}, function(err, result) {
    collection.update({mykey:2}, {$push:{docs:{doc2:1}}}, {w:1}, function(err, result) {});
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alright before we look at the code we want to understand how document updates work and how to do the efficiently. The most basic and less efficient way is to replace the whole document, this is not really the way to go if you want to change just a field in your document. Luckily Mongo DB provides a whole set of operations that let you modify just pieces of the document &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Atomic+Operations&#34;&gt;Atomic operations documentation&lt;/a&gt;. Basically outlined below.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$inc - increment a particular value by a certain amount&lt;/li&gt;
&lt;li&gt;$set - set a particular value&lt;/li&gt;
&lt;li&gt;$unset - delete a particular field (v1.3+)&lt;/li&gt;
&lt;li&gt;$push - append a value to an array&lt;/li&gt;
&lt;li&gt;$pushAll - append several values to an array&lt;/li&gt;
&lt;li&gt;$addToSet - adds value to the array only if its not in the array already&lt;/li&gt;
&lt;li&gt;$pop - removes the last element in an array&lt;/li&gt;
&lt;li&gt;$pull - remove a value(s) from an existing array&lt;/li&gt;
&lt;li&gt;$pullAll - remove several value(s) from an existing array&lt;/li&gt;
&lt;li&gt;$rename - renames the field&lt;/li&gt;
&lt;li&gt;$bit - bitwise operations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that the operations are outline let&amp;rsquo;s dig into the specific cases show in the code example.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.update({mykey:1}, {$set:{fieldtoupdate:2}}, {w:1}, function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right so this update will look for the document that has a field &lt;strong&gt;mykey&lt;/strong&gt; equal to &lt;strong&gt;1&lt;/strong&gt; and apply an update to the field &lt;strong&gt;fieldtoupdate&lt;/strong&gt; setting the value to &lt;strong&gt;2&lt;/strong&gt;. Since we are using the &lt;strong&gt;{w:1}&lt;/strong&gt; option the result parameter in the callback will return the value &lt;strong&gt;1&lt;/strong&gt; indicating that 1 document was modified by the update statement.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.update({mykey:2}, {$push:{docs:{doc2:1}}}, {w:1}, function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This updates adds another document to the field &lt;strong&gt;docs&lt;/strong&gt; in the document identified by &lt;strong&gt;{mykey:2}&lt;/strong&gt; using the atomic operation &lt;strong&gt;$push&lt;/strong&gt;. This allows you to modify keep such structures as queues in Mongo DB.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at the remove operation for the driver. As before let&amp;rsquo;s start with a piece of code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) { return console.dir(err); }

  var collection = db.collection(&#39;test&#39;);
  var docs = [{mykey:1}, {mykey:2}, {mykey:3}];

  collection.insert(docs, {w:1}, function(err, result) {

    collection.remove({mykey:1});

    collection.remove({mykey:2}, {w:1}, function(err, result) {});

    collection.remove();
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s examine the 3 remove variants and what they do.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.remove({mykey:1});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This leverages the fact that Mongo DB is asynchronous and that it does not return a result for &lt;strong&gt;insert/update/remove&lt;/strong&gt; to allow for &lt;strong&gt;synchronous&lt;/strong&gt; style execution. This particular remove query will remove the document where &lt;strong&gt;mykey&lt;/strong&gt; equals &lt;strong&gt;1&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.remove({mykey:2}, {w:1}, function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This remove statement removes the document where &lt;strong&gt;mykey&lt;/strong&gt; equals &lt;strong&gt;2&lt;/strong&gt; but since we are using &lt;strong&gt;{w:1}&lt;/strong&gt; it will back to Mongo DB to get the status of the remove operation and return the number of documents removed in the result variable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.remove();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This last one will remove all documents in the collection.&lt;/p&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Time to Query&lt;/h2&gt;

&lt;p&gt;Queries is of course a fundamental part of interacting with a database and Mongo DB is no exception. Fortunately for us it has a rich query interface with cursors and close to SQL concepts for slicing and dicing your datasets. To build queries we have lots of operators to choose from &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Advanced+Queries&#34;&gt;Mongo DB advanced queries&lt;/a&gt;. There are literarily tons of ways to search and ways to limit the query. Let&amp;rsquo;s look at some simple code for dealing with queries in different ways.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Retrieve
var MongoClient = require(&#39;mongodb&#39;).MongoClient;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) { return console.dir(err); }

  var collection = db.collection(&#39;test&#39;);
  var docs = [{mykey:1}, {mykey:2}, {mykey:3}];

  collection.insert(docs, {w:1}, function(err, result) {

    collection.find().toArray(function(err, items) {});

    var stream = collection.find({mykey:{$ne:2}}).stream();
    stream.on(&amp;quot;data&amp;quot;, function(item) {});
    stream.on(&amp;quot;end&amp;quot;, function() {});

    collection.findOne({mykey:1}, function(err, item) {});

  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we start picking apart the code there is one thing that needs to be understood, the &lt;strong&gt;find&lt;/strong&gt; method does not execute the actual query. It builds an instance of &lt;strong&gt;Cursor&lt;/strong&gt; that you then use to retrieve the data. This lets you manage how you retrieve the data from Mongo DB and keeps state about your current Cursor state on Mongo DB. Now let&amp;rsquo;s pick apart the queries we have here and look at what they do.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.find().toArray(function(err, items) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This query will fetch all the document in the collection and return them as an array of items. Be careful with the function &lt;strong&gt;toArray&lt;/strong&gt; as it might cause a lot of memory usage as it will instantiate all the document into memory before returning the final array of items. If you have a big resultset you could run into memory issues.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var stream = collection.find({mykey:{$ne:2}}).stream();
stream.on(&amp;quot;data&amp;quot;, function(item) {});
stream.on(&amp;quot;end&amp;quot;, function() {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the preferred way if you have to retrieve a lot of data for streaming, as data is deserialized a &lt;strong&gt;data&lt;/strong&gt; event is emitted. This keeps the resident memory usage low as the documents are streamed to you. Very useful if you are pushing documents out via websockets or some other streaming socket protocol. Once there is no more document the driver will emit the &lt;strong&gt;end&lt;/strong&gt; event to notify the application that it&amp;rsquo;s done.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;collection.findOne({mykey:1}, function(err, item) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is special supported function to retrieve just one specific document bypassing the need for a cursor object.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s pretty much it for the quick intro on how to use the database. I have also included a list of links to where to go to find more information and also a sample crude location application I wrote using express JS and mongo DB.&lt;/p&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Links and stuff&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/examples&#34;&gt;The driver examples, good starting point for basic usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/test&#34;&gt;All the integration tests, they have tons of different usage cases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mongodb.org/display/DOCS/Advanced+Queries&#34;&gt;The Mongo DB wiki pages such as the advanced query link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/christkv/mongodb-presentation&#34;&gt;A silly simple location based application using Express JS and Mongo DB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MongoDB Native: The Official Node.js MongoDB driver</title>
      <link>http://christiankvalheim.com/project/mongodb-native</link>
      <pubDate>Wed, 01 Sep 2010 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/project/mongodb-native</guid>
      <description>

&lt;p&gt;The Node.js MongoDB driver was started in 2009 and has grown to become one of the most popular database drivers for the Node.js Platform.&lt;/p&gt;

&lt;p&gt;MongoDB is a document database that allows the storage of documents in json format. It&amp;rsquo;s main benefits come from it&amp;rsquo;s ability to store unstructured data while maintaining traditional databases ability to query and index the document efficiently. MongoDb also provides a whole scaling up path.&lt;/p&gt;

&lt;p&gt;A whole ecosystem has sprung out on top of the driver including ODM mappers such as Mongoose and whole frameworks such as Meteor (a fantastic real time framework). The driver is constantly evolving and there are some future spin-off projects planned around the driver.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;td&gt;what&lt;/td&gt;
&lt;td&gt;where&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;documentation&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://mongodb.github.io/node-mongodb-native/&#34;&gt;http://mongodb.github.io/node-mongodb-native/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;apidoc&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://mongodb.github.io/node-mongodb-native/&#34;&gt;http://mongodb.github.io/node-mongodb-native/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;source&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native&#34;&gt;https://github.com/mongodb/node-mongodb-native&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;mongodb&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://www.mongodb.org/&#34;&gt;http://www.mongodb.org/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Bugs / Feature Requests&lt;/h3&gt;

&lt;p&gt;Think youve found a bug? Want to see a new feature in node-mongodb-native? Please open a
case in our issue management tool, JIRA:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Create an account and login &lt;a href=&#34;https://jira.mongodb.org&#34;&gt;https://jira.mongodb.org&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Navigate to the NODE project &lt;a href=&#34;https://jira.mongodb.org/browse/NODE&#34;&gt;https://jira.mongodb.org/browse/NODE&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Click &lt;strong&gt;Create Issue&lt;/strong&gt; - Please provide as much information as possible about the issue type and how to reproduce it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bug reports in JIRA for all driver projects (i.e. NODE, PYTHON, CSHARP, JAVA) and the
Core Server (i.e. SERVER) project are &lt;strong&gt;public&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Questions and Bug Reports&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;mailing list: &lt;a href=&#34;https://groups.google.com/forum/#!forum/node-mongodb-native&#34;&gt;https://groups.google.com/forum/#!forum/node-mongodb-native&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;jira: &lt;a href=&#34;http://jira.mongodb.org/&#34;&gt;http://jira.mongodb.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Change Log&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://jira.mongodb.org/browse/NODE&#34;&gt;http://jira.mongodb.org/browse/NODE&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Install&lt;/h2&gt;

&lt;p&gt;To install the most recent release from npm, run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install mongodb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That may give you a warning telling you that bugs[&amp;lsquo;web&amp;rsquo;] should be bugs[&amp;lsquo;url&amp;rsquo;], it would be safe to ignore it (this has been fixed in the development version)&lt;/p&gt;

&lt;p&gt;To install the latest from the repository, run::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install path/to/node-mongodb-native
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Live Examples&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://runnable.com/node-mongodb-native&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://runnable.com/external/styles/assets/runnablebtn.png&#34; style=&#34;width:67px;height:25px;&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is a node.js driver for MongoDB. It&amp;rsquo;s a port (or close to a port) of the library for ruby at &lt;a href=&#34;http://github.com/mongodb/mongo-ruby-driver/&#34;&gt;http://github.com/mongodb/mongo-ruby-driver/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A simple example of inserting a document.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  var MongoClient = require(&#39;mongodb&#39;).MongoClient
    , format = require(&#39;util&#39;).format;

  MongoClient.connect(&#39;mongodb://127.0.0.1:27017/test&#39;, function(err, db) {
    if(err) throw err;

    var collection = db.collection(&#39;test_insert&#39;);
    collection.insert({a:2}, function(err, docs) {
      
      collection.count(function(err, count) {
        console.log(format(&amp;quot;count = %s&amp;quot;, count));
      });

      // Locate all the entries using find
      collection.find().toArray(function(err, results) {
        console.dir(results);
        // Let&#39;s close the db
        db.close();
      });
    });
  })
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Data types&lt;/h2&gt;

&lt;p&gt;To store and retrieve the non-JSON MongoDb primitives (&lt;a href=&#34;http://www.mongodb.org/display/DOCS/Object+IDs&#34;&gt;ObjectID&lt;/a&gt;, Long, Binary, &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Timestamp+data+type&#34;&gt;Timestamp&lt;/a&gt;, &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Database+References#DatabaseReferences-DBRef&#34;&gt;DBRef&lt;/a&gt;, Code).&lt;/p&gt;

&lt;p&gt;In particular, every document has a unique &lt;code&gt;_id&lt;/code&gt; which can be almost any type, and by default a 12-byte ObjectID is created. ObjectIDs can be represented as 24-digit hexadecimal strings, but you must convert the string back into an ObjectID before you can use it in the database. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  // Get the objectID type
  var ObjectID = require(&#39;mongodb&#39;).ObjectID;

  var idString = &#39;4e4e1638c85e808431000003&#39;;
  collection.findOne({_id: new ObjectID(idString)}, console.log)  // ok
  collection.findOne({_id: idString}, console.log)  // wrong! callback gets undefined
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here are the constructors the non-Javascript BSON primitive types:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  // Fetch the library
  var mongo = require(&#39;mongodb&#39;);
  // Create new instances of BSON types
  new mongo.Long(numberString)
  new mongo.ObjectID(hexString)
  new mongo.Timestamp()  // the actual unique number is generated on insert.
  new mongo.DBRef(collectionName, id, dbName)
  new mongo.Binary(buffer)  // takes a string or Buffer
  new mongo.Code(code, [context])
  new mongo.Symbol(string)
  new mongo.MinKey()
  new mongo.MaxKey()
  new mongo.Double(number)  // Force double storage
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_7&#34;&gt;The C/C++ bson parser/serializer&lt;/h3&gt;

&lt;p&gt;If you are running a version of this library has the C/C++ parser compiled, to enable the driver to use the C/C++ bson parser pass it the option native_parser:true like below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  // using native_parser:
  MongoClient.connect(&#39;mongodb://127.0.0.1:27017/test&#39;
    , {db: {native_parser: true}}, function(err, db) {})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The C++ parser uses the js objects both for serialization and deserialization.&lt;/p&gt;

&lt;h2 id=&#34;toc_8&#34;&gt;GitHub information&lt;/h2&gt;

&lt;p&gt;The source code is available at &lt;a href=&#34;http://github.com/mongodb/node-mongodb-native&#34;&gt;http://github.com/mongodb/node-mongodb-native&lt;/a&gt;.
You can either clone the repository or download a tarball of the latest release.&lt;/p&gt;

&lt;p&gt;Once you have the source you can test the driver by running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ make test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;in the main directory. You will need to have a mongo instance running on localhost for the integration tests to pass.&lt;/p&gt;

&lt;h2 id=&#34;toc_9&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;For examples look in the examples/ directory. You can execute the examples using node.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd examples
$ node queries.js
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_10&#34;&gt;GridStore&lt;/h2&gt;

&lt;p&gt;The GridStore class allows for storage of binary files in mongoDB using the mongoDB defined files and chunks collection definition.&lt;/p&gt;

&lt;p&gt;For more information have a look at &lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/blob/master/docs/gridfs.md&#34;&gt;Gridstore&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_11&#34;&gt;Replicasets&lt;/h2&gt;

&lt;p&gt;For more information about how to connect to a replicaset have a look at the extensive documentation &lt;a href=&#34;http://mongodb.github.com/node-mongodb-native/&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;toc_12&#34;&gt;Primary Key Factories&lt;/h3&gt;

&lt;p&gt;Defining your own primary key factory allows you to generate your own series of id&amp;rsquo;s
(this could f.ex be to use something like ISBN numbers). The generated the id needs to be a 12 byte long &amp;ldquo;string&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Simple example below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  var MongoClient = require(&#39;mongodb&#39;).MongoClient
    , format = require(&#39;util&#39;).format;    

  // Custom factory (need to provide a 12 byte array);
  CustomPKFactory = function() {}
  CustomPKFactory.prototype = new Object();
  CustomPKFactory.createPk = function() {
    return new ObjectID(&amp;quot;aaaaaaaaaaaa&amp;quot;);
  }

  MongoClient.connect(&#39;mongodb://127.0.0.1:27017/test&#39;, {&#39;pkFactory&#39;:CustomPKFactory}, function(err, db) {
    if(err) throw err;

    db.dropDatabase(function(err, done) {
      
      db.createCollection(&#39;test_custom_key&#39;, function(err, collection) {
        
        collection.insert({&#39;a&#39;:1}, function(err, docs) {
          
          collection.find({&#39;_id&#39;:new ObjectID(&amp;quot;aaaaaaaaaaaa&amp;quot;)}).toArray(function(err, items) {
            console.dir(items);
            // Let&#39;s close the db
            db.close();
          });
        });
      });
    });
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_13&#34;&gt;Documentation&lt;/h2&gt;

&lt;p&gt;If this document doesn&amp;rsquo;t answer your questions, see the source of
&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/blob/master/lib/mongodb/collection.js&#34;&gt;Collection&lt;/a&gt;
or &lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/blob/master/lib/mongodb/cursor.js&#34;&gt;Cursor&lt;/a&gt;,
or the documentation at MongoDB for query and update formats.&lt;/p&gt;

&lt;h3 id=&#34;toc_14&#34;&gt;Find&lt;/h3&gt;

&lt;p&gt;The find method is actually a factory method to create
Cursor objects. A Cursor lazily uses the connection the first time
you call &lt;code&gt;nextObject&lt;/code&gt;, &lt;code&gt;each&lt;/code&gt;, or &lt;code&gt;toArray&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The basic operation on a cursor is the &lt;code&gt;nextObject&lt;/code&gt; method
that fetches the next matching document from the database. The convenience
methods &lt;code&gt;each&lt;/code&gt; and &lt;code&gt;toArray&lt;/code&gt; call &lt;code&gt;nextObject&lt;/code&gt; until the cursor is exhausted.&lt;/p&gt;

&lt;p&gt;Signatures:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  var cursor = collection.find(query, [fields], options);
  cursor.sort(fields).limit(n).skip(m).

  cursor.nextObject(function(err, doc) {});
  cursor.each(function(err, doc) {});
  cursor.toArray(function(err, docs) {});

  cursor.rewind()  // reset the cursor to its initial state.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Useful chainable methods of cursor. These can optionally be options of &lt;code&gt;find&lt;/code&gt; instead of method calls:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.limit(n).skip(m)&lt;/code&gt; to control paging.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.sort(fields)&lt;/code&gt; Order by the given fields. There are several equivalent syntaxes:&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.sort({field1: -1, field2: 1})&lt;/code&gt; descending by field1, then ascending by field2.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.sort([[&#39;field1&#39;, &#39;desc&#39;], [&#39;field2&#39;, &#39;asc&#39;]])&lt;/code&gt; same as above&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.sort([[&#39;field1&#39;, &#39;desc&#39;], &#39;field2&#39;])&lt;/code&gt; same as above&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.sort(&#39;field1&#39;)&lt;/code&gt; ascending by field1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other options of &lt;code&gt;find&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fields&lt;/code&gt; the fields to fetch (to avoid transferring the entire document)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tailable&lt;/code&gt; if true, makes the cursor &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Tailable+Cursors&#34;&gt;tailable&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;batchSize&lt;/code&gt; The number of the subset of results to request the database
to return for every request. This should initially be greater than 1 otherwise
the database will automatically close the cursor. The batch size can be set to 1
with &lt;code&gt;batchSize(n, function(err){})&lt;/code&gt; after performing the initial query to the database.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hint&lt;/code&gt; See &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Optimization#Optimization-Hint&#34;&gt;Optimization: hint&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;explain&lt;/code&gt; turns this into an explain query. You can also call
&lt;code&gt;explain()&lt;/code&gt; on any cursor to fetch the explanation.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;snapshot&lt;/code&gt; prevents documents that are updated while the query is active
from being returned multiple times. See more
&lt;a href=&#34;http://www.mongodb.org/display/DOCS/How+to+do+Snapshotted+Queries+in+the+Mongo+Database&#34;&gt;details about query snapshots&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;timeout&lt;/code&gt; if false, asks MongoDb not to time out this cursor after an
inactivity period.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For information on how to create queries, see the
&lt;a href=&#34;http://www.mongodb.org/display/DOCS/Querying&#34;&gt;MongoDB section on querying&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  var MongoClient = require(&#39;mongodb&#39;).MongoClient
    , format = require(&#39;util&#39;).format;    

  MongoClient.connect(&#39;mongodb://127.0.0.1:27017/test&#39;, function(err, db) {
    if(err) throw err;

    var collection = db
      .collection(&#39;test&#39;)
      .find({})
      .limit(10)
      .toArray(function(err, docs) {
        console.dir(docs);
    });
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_15&#34;&gt;Insert&lt;/h3&gt;

&lt;p&gt;Signature:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  collection.insert(docs, options, [callback]);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;docs&lt;/code&gt; can be a single document or an array of documents.&lt;/p&gt;

&lt;p&gt;Useful options:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;safe:true&lt;/code&gt; Should always set if you have a callback.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See also: &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Inserting&#34;&gt;MongoDB docs for insert&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  var MongoClient = require(&#39;mongodb&#39;).MongoClient
    , format = require(&#39;util&#39;).format;    

  MongoClient.connect(&#39;mongodb://127.0.0.1:27017/test&#39;, function(err, db) {
    if(err) throw err;
    
    db.collection(&#39;test&#39;).insert({hello: &#39;world&#39;}, {w:1}, function(err, objects) {
      if (err) console.warn(err.message);
      if (err &amp;amp;&amp;amp; err.message.indexOf(&#39;E11000 &#39;) !== -1) {
        // this _id was already inserted in the database
      }
    });
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that there&amp;rsquo;s no reason to pass a callback to the insert or update commands
unless you use the &lt;code&gt;safe:true&lt;/code&gt; option. If you don&amp;rsquo;t specify &lt;code&gt;safe:true&lt;/code&gt;, then
your callback will be called immediately.&lt;/p&gt;

&lt;h3 id=&#34;toc_16&#34;&gt;Update: update and insert (upsert)&lt;/h3&gt;

&lt;p&gt;The update operation will update the first document that matches your query
(or all documents that match if you use &lt;code&gt;multi:true&lt;/code&gt;).
If &lt;code&gt;safe:true&lt;/code&gt;, &lt;code&gt;upsert&lt;/code&gt; is not set, and no documents match, your callback will return 0 documents updated.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Updating&#34;&gt;MongoDB docs&lt;/a&gt; for
the modifier (&lt;code&gt;$inc&lt;/code&gt;, &lt;code&gt;$set&lt;/code&gt;, &lt;code&gt;$push&lt;/code&gt;, etc.) formats.&lt;/p&gt;

&lt;p&gt;Signature:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  collection.update(criteria, objNew, options, [callback]);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Useful options:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;safe:true&lt;/code&gt; Should always set if you have a callback.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;multi:true&lt;/code&gt; If set, all matching documents are updated, not just the first.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;upsert:true&lt;/code&gt; Atomically inserts the document if no documents matched.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example for &lt;code&gt;update&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  var MongoClient = require(&#39;mongodb&#39;).MongoClient
    , format = require(&#39;util&#39;).format;    

  MongoClient.connect(&#39;mongodb://127.0.0.1:27017/test&#39;, function(err, db) {
    if(err) throw err;

    db.collection(&#39;test&#39;).update({hi: &#39;here&#39;}, {$set: {hi: &#39;there&#39;}}, {w:1}, function(err) {
      if (err) console.warn(err.message);
      else console.log(&#39;successfully updated&#39;);
    });
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_17&#34;&gt;Find and modify&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;findAndModify&lt;/code&gt; is like &lt;code&gt;update&lt;/code&gt;, but it also gives the updated document to
your callback. But there are a few key differences between findAndModify and
update:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The signatures differ.&lt;/li&gt;
&lt;li&gt;You can only findAndModify a single item, not multiple items.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Signature:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;    collection.findAndModify(query, sort, update, options, callback)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The sort parameter is used to specify which object to operate on, if more than
one document matches. It takes the same format as the cursor sort (see
Connection.find above).&lt;/p&gt;

&lt;p&gt;See the
&lt;a href=&#34;http://www.mongodb.org/display/DOCS/findAndModify+Command&#34;&gt;MongoDB docs for findAndModify&lt;/a&gt;
for more details.&lt;/p&gt;

&lt;p&gt;Useful options:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;remove:true&lt;/code&gt; set to a true to remove the object before returning&lt;/li&gt;
&lt;li&gt;&lt;code&gt;new:true&lt;/code&gt; set to true if you want to return the modified object rather than the original. Ignored for remove.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;upsert:true&lt;/code&gt; Atomically inserts the document if no documents matched.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example for &lt;code&gt;findAndModify&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;javascript&#34;&gt;  var MongoClient = require(&#39;mongodb&#39;).MongoClient
    , format = require(&#39;util&#39;).format;    

  MongoClient.connect(&#39;mongodb://127.0.0.1:27017/test&#39;, function(err, db) {
    if(err) throw err;
    db.collection(&#39;test&#39;).findAndModify({hello: &#39;world&#39;}, [[&#39;_id&#39;,&#39;asc&#39;]], {$set: {hi: &#39;there&#39;}}, {}, function(err, object) {
      if (err) console.warn(err.message);
      else console.dir(object);  // undefined if no matching object exists.
    });
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_18&#34;&gt;Save&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;save&lt;/code&gt; method is a shorthand for upsert if the document contains an
&lt;code&gt;_id&lt;/code&gt;, or an insert if there is no &lt;code&gt;_id&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;toc_19&#34;&gt;Release Notes&lt;/h2&gt;

&lt;p&gt;See HISTORY&lt;/p&gt;

&lt;h2 id=&#34;toc_20&#34;&gt;Credits&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://github.com/mongodb/mongo-ruby-driver/&#34;&gt;10gen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://code.google.com/closure/library/&#34;&gt;Google Closure Library&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://jsfromhell.com/classes/binary-parser&#34;&gt;Jonas Raoni Soares Silva&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;toc_21&#34;&gt;Contributors&lt;/h2&gt;

&lt;p&gt;Aaron Heckmann, Christoph Pojer, Pau Ramon Revilla, Nathan White, Emmerman, Seth LaForge, Boris Filipov, Stefan Schrmeli, Tedde Lundgren, renctan, Sergey Ukustov, Ciaran Jessup, kuno, srimonti, Erik Abele, Pratik Daga, Slobodan Utvic, Kristina Chodorow, Yonathan Randolph, Brian Noguchi, Sam Epstein, James Harrison Fisher, Vladimir Dronnikov, Ben Hockey, Henrik Johansson, Simon Weare, Alex Gorbatchev, Shimon Doodkin, Kyle Mueller, Eran Hammer-Lahav, Marcin Ciszak, Franois de Metz, Vinay Pulim, nstielau, Adam Wiggins, entrinzikyl, Jeremy Selier, Ian Millington, Public Keating, andrewjstone, Christopher Stott, Corey Jewett, brettkiefer, Rob Holland, Senmiao Liu, heroic, gitfy&lt;/p&gt;

&lt;h2 id=&#34;toc_22&#34;&gt;License&lt;/h2&gt;

&lt;p&gt;Copyright 2009 - 2013 MongoDb Inc.&lt;/p&gt;

&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &amp;ldquo;License&amp;rdquo;);
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   http://www.apache.org/licenses/LICENSE-2.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an &amp;ldquo;AS IS&amp;rdquo; BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
