<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Gridfs on Christian Kvalheim </title>
    <link>http://christiankvalheim.com/topics/gridfs/index.xml</link>
    <language>en-us</language>
    <author>Christian Kvalheim</author>
    <rights>Copyright (c) 2011 - 2014, Christian Kvalheim; all rights reserved.</rights>
    <updated>Sun, 16 Mar 2014 00:00:00 UTC</updated>
    
    <item>
      <title>Mongo Driver and Mongo DB 2.6 Features</title>
      <link>http://christiankvalheim.com/post/an_introduction_to_1_4_and_2_6</link>
      <pubDate>Sun, 16 Mar 2014 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/an_introduction_to_1_4_and_2_6</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;Mongo Driver and Mongo DB 2.6 Features&lt;/h1&gt;

&lt;p&gt;MongoDB 2.6 introduces some new powerful features that are reflected in the 1.4 driver release. These include.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Aggregation cursors&lt;/li&gt;
&lt;li&gt;Per query timeouts &lt;strong&gt;maxTimeMS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Ordered and Unordered bulk operations&lt;/li&gt;
&lt;li&gt;A parallelCollectionScan command for fast reading of an entire collection&lt;/li&gt;
&lt;li&gt;Integrated text search in the query language&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moreover the driver includes a whole slew of minor and major bug fixes and features. Some of the more noteworthy features include.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Better support for domains in node.js&lt;/li&gt;
&lt;li&gt;Reconnect events for replicaset and mongos connections&lt;/li&gt;
&lt;li&gt;Replicaset emits &amp;ldquo;joined&amp;rdquo; and &amp;ldquo;left&amp;rdquo; events when new server join or leave the set&lt;/li&gt;
&lt;li&gt;Added &lt;strong&gt;bufferMaxEntries&lt;/strong&gt; entry to allow tuning on how long driver keeps waiting for servers to come back up (default is until memory exhaustion)&lt;/li&gt;
&lt;li&gt;Upgraded BSON parser to rely on 0.2.6 returning to using &lt;strong&gt;nan&lt;/strong&gt; package&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s look at the main things in 2.6 features one by one.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Aggregation cursors&lt;/h2&gt;

&lt;p&gt;The addition off aggregation cursors to MongoDB 2.6 now means that applications can disregard the previous max result limit of 16MB. Let&amp;rsquo;s look at a simple use of the aggregation cursor.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get an aggregation cursor
  var cursor = db.collection(&#39;data&#39;).aggregate([
      {$match: {}}
    ], {
        allowDiskUsage: true
      , cursor: {batchSize: 1000}   
    });

  // Use cursor as stream
  cursor.on(&#39;data&#39;, function(data) {
    console.dir(data);
  });

  cursor.on(&#39;end&#39;, function() {
    db.close();
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As one can see the cursor implements the &lt;strong&gt;Readable&lt;/strong&gt; stream interface for 0.10.X or higher. For 2.4 the driver will emulate the cursor behavior by wrapping the result document.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;maxTimeMS&lt;/h2&gt;

&lt;p&gt;One feature that has requested often is the ability to timeout individual queries. In MongoDB 2.6 it&amp;rsquo;s finally arrived and is known as the &lt;strong&gt;maxTimeMS&lt;/strong&gt; option. Let&amp;rsquo;s take a look at a simple usage of the property with a query.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get an aggregation cursor
  var cursor = db.collection(&#39;data&#39;)
    .find(&amp;quot;$where&amp;quot;: &amp;quot;sleep(1000) || true&amp;quot;)
    .maxTimeMS(50);

  // Get alll the items
  cursor.toArray(function(err, items) {
    console.dir(err);
    console.dir(items);
    db.close();
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a bit of a contrived example using sleep to force the query to wait a second. With the &lt;strong&gt;maxTimeMS&lt;/strong&gt; set to 50 milliseconds the query will be aborted before the full second is up.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Ordered/Unordered bulk operations&lt;/h2&gt;

&lt;p&gt;Under the covers MongoDB is moving away from the combination of a write operation + get last error (GLE) and towards a write commands api. These new commands allow for the execution of bulk insert/update/remove operations. The bulk api&amp;rsquo;s are abstractions on top of this that server to make it easy to build bulk operations. Bulk operations come in two main flavors.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Ordered bulk operations. These operations execute all the operation in order and error out on the first write error.&lt;/li&gt;
&lt;li&gt;Unordered bulk operations. These operations execute all the operations in parallel and aggregates up all the errors. Unordered bulk operations do not guarantee order of execution.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s look at two simple examples using ordered and unordered operations.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get the collection
  var col = db.collection(&#39;batch_write_ordered_ops&#39;);
  // Initialize the Ordered Batch
  var batch = col.initializeOrderedBulkOp();

  // Add some operations to be executed in order
  batch.insert({a:1});
  batch.find({a:1}).updateOne({$set: {b:1}});
  batch.find({a:2}).upsert().updateOne({$set: {b:2}});
  batch.insert({a:3});
  batch.find({a:3}).remove({a:3});

  // Execute the operations
  batch.execute(function(err, result) {
    console.dir(err);
    console.dir(result);    
    db.close();
  });
});

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get the collection
  var col = db.collection(&#39;batch_write_ordered_ops&#39;);
  // Initialize the Ordered Batch
  var batch = col.initializeUnorderedBulkOp();

  // Add some operations to be executed in order
  batch.insert({a:1});
  batch.find({a:1}).updateOne({$set: {b:1}});
  batch.find({a:2}).upsert().updateOne({$set: {b:2}});
  batch.insert({a:3});
  batch.find({a:3}).remove({a:3});

  // Execute the operations
  batch.execute(function(err, result) {
    console.dir(err);
    console.dir(result);    
    db.close();
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For older servers than 2.6 the API will downconvert the operations. However it&amp;rsquo;s not possible to downconvert 100% so there might be slight edge cases where it cannot correctly report the right numbers.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;parallelCollectionScan&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;parallelCollectionScan&lt;/strong&gt; command is a special command targeted at reading out an entire collection using &lt;strong&gt;numCursors&lt;/strong&gt; parallel cursors.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get an aggregation cursor
  db.collection(&#39;data&#39;).parallelCollectionScan({numCursors:3}, function(err, cursors) {
    var results = [];

    for(var i = 0; i &amp;lt; cursors.length; i++) {
      cursors[i].get(function(err, items) {
        test.equal(err, null);

        // Add docs to results array
        results = results.concat(items);
        numCursors = numCursors - 1;

        // No more cursors let&#39;s ensure we got all results
        if(numCursors == 0) {
          test.equal(docs.length, results.length);

          db.close();
          test.done();
        }
      });
    }
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This optimizes the IO throughput from a collection.&lt;/p&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Integrated text search in the query language&lt;/h2&gt;

&lt;p&gt;Text indexes are now integrated into the main query language and enabled by default. A simple example.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, function(err, db) {
  // Get the collection
  var collection = db.collection(&#39;textSearchWithSort&#39;);
  collection.ensureIndex({s: &#39;text&#39;}, function(err, result) {
    test.equal(null, err);

    collection.insert([
        {s: &#39;spam&#39;}
      , {s: &#39;spam eggs and spam&#39;}
      , {s: &#39;sausage and eggs&#39;}], function(err, result) {
        test.equal(null, err);

        collection.find(
            {$text: {$search: &#39;spam&#39;}}
          , {fields: {_id: false, s: true, score: {$meta: &#39;textScore&#39;}}}
        ).sort({score: {$meta: &#39;textScore&#39;}}).toArray(function(err, items) {
          test.equal(null, err);
          test.equal(&amp;quot;spam eggs and spam&amp;quot;, items[0].s);
          db.close();
          test.done();
        });
      });
  });      
});
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Emitting Reconnect and Joined/Left events&lt;/h2&gt;

&lt;p&gt;The Replicaset and Mongos now emits events for servers joining and leaving the replicaset. This let&amp;rsquo;s applications more easily monitor the changes in the driver over time. &lt;strong&gt;Reconnect&lt;/strong&gt; in the context of a Replicaset or Mongos means that the driver is starting to replay buffered operations.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017,localhost:27027/test&amp;quot;, function(err, db) {
  db.serverConfig.on(&#39;joined&#39;, function(err, server) {
    console.log(&amp;quot;server joined&amp;quot;);
    console.dir(server);
  });

  db.serverConfig.on(&#39;left&#39;, function(err, server) {
    console.log(&amp;quot;server left&amp;quot;);
    console.dir(server);
  });

  db.serverConfig.on(&#39;reconnect&#39;, function() {
    console.log(&amp;quot;server reconnected&amp;quot;);
  }); 
});
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_7&#34;&gt;bufferMaxEntries&lt;/h2&gt;

&lt;p&gt;Buffered Max Entries allow for more fine grained control on how many operations that will be buffered before the driver errors out and stops attempting to reconnect.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient;

MongoClient.connect(&amp;quot;mongodb://localhost:27017/test&amp;quot;, {
    db: {bufferMaxEntries:0},
  }, function(err, db) {
    db.close();
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example disables the command buffering completely and errors out the moment there is no connection available. The default value (for backward compatibility) is to buffer until memory runs out. Be aware that by setting a very low value you can cause some problems in failover scenarios in Replicasets as it might take a little but of time before f.ex a new Primary is elected and steps up to accept writes. Setting &lt;strong&gt;bufferMaxEntries&lt;/strong&gt; to 0 in this case will cause the driver to error out instead of falling over correctly.&lt;/p&gt;

&lt;h2 id=&#34;toc_8&#34;&gt;Fsync and journal Write Concerns note&lt;/h2&gt;

&lt;p&gt;MongoDB from version 2.6 and higher disallows the combination of &lt;strong&gt;journal&lt;/strong&gt; and &lt;strong&gt;fsync&lt;/strong&gt;. Combining them will cause an error while on 2.4 &lt;strong&gt;fsync&lt;/strong&gt; was ignored when provided with &lt;strong&gt;journal&lt;/strong&gt;. The following semantics apply.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;j: If true block until write operations have been committed to the journal. Cannot be used in combination with &lt;code&gt;fsync&lt;/code&gt;. Prior to MongoDB 2.6 this option was ignored if the server was running without journaling. Starting with MongoDB 2.6 write operations will fail with an exception if this option is used when the server is running without journaling.&lt;/li&gt;
&lt;li&gt;fsync: If true and the server is running without journaling, blocks until the server has synced all data files to disk. If the server is running with journaling, this acts the same as the &lt;code&gt;j&lt;/code&gt; option, blocking until write operations have been committed to the journal. Cannot be used in combination with &lt;code&gt;j&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A primer for GridFS using the Mongo DB driver</title>
      <link>http://christiankvalheim.com/post/a_primer_for_gridfs_using_the_mongodb_driver</link>
      <pubDate>Wed, 02 Mar 2011 00:00:00 UTC</pubDate>
      <author>Christian Kvalheim</author>
      <guid>http://christiankvalheim.com/post/a_primer_for_gridfs_using_the_mongodb_driver</guid>
      <description>

&lt;h1 id=&#34;toc_0&#34;&gt;A primer for GridFS using the Mongo DB driver&lt;/h1&gt;

&lt;p&gt;In the first tutorial we targeted general usage of the database. But Mongo DB is much more than this. One of the additional very useful features is to act as a file storage system. This is accomplish in Mongo by having a file collection and a chunks collection where each document in the chunks collection makes up a &lt;strong&gt;Block&lt;/strong&gt; of the file. In this tutorial we will look at how to use the GridFS functionality and what functions are available.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;A simple example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s dive straight into a simple example on how to write a file to the grid using the simplified Grid class.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  Grid = mongo.Grid;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) return console.dir(err);

  var grid = new Grid(db, &#39;fs&#39;);    
  var buffer = new Buffer(&amp;quot;Hello world&amp;quot;);
  grid.put(buffer, {metadata:{category:&#39;text&#39;}, content_type: &#39;text&#39;}, function(err, fileInfo) {
    if(!err) {
      console.log(&amp;quot;Finished writing file to Mongo&amp;quot;);
    }
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All right let&amp;rsquo;s dissect the example. The first thing you&amp;rsquo;ll notice is the statement&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var grid = new Grid(db, &#39;fs&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since GridFS is actually a special structure stored as collections you&amp;rsquo;ll notice that we are using the db connection that we used in the previous tutorial to operate on collections and documents. The second parameter &lt;strong&gt;&amp;lsquo;fs&amp;rsquo;&lt;/strong&gt; allows you to change the collections you want to store the data in. In this example the collections would be &lt;strong&gt;fs_files&lt;/strong&gt; and &lt;strong&gt;fs_chunks&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Having a live grid instance we now go ahead and create some test data stored in a Buffer instance, although you can pass in a string instead. We then write our data to disk.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var buffer = new Buffer(&amp;quot;Hello world&amp;quot;);
grid.put(buffer, {metadata:{category:&#39;text&#39;}, content_type: &#39;text&#39;}, function(err, fileInfo) {
  if(!err) {
    console.log(&amp;quot;Finished writing file to Mongo&amp;quot;);
  }
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s deconstruct the call we just made. The &lt;strong&gt;put&lt;/strong&gt; call will write the data you passed in as one or more chunks. The second parameter is a hash of options for the Grid class. In this case we wish to annotate the file we are writing to Mongo DB with some metadata and also specify a content type. Each file entry in GridFS has support for metadata documents which might be very useful if you are for example storing images in you Mongo DB and need to store all the data associated with the image.&lt;/p&gt;

&lt;p&gt;One important thing is to take not that the put method return a document containing a &lt;strong&gt;_id&lt;/strong&gt;, this is an &lt;strong&gt;ObjectID&lt;/strong&gt; identifier that you&amp;rsquo;ll need to use if you wish to retrieve the file contents later.&lt;/p&gt;

&lt;p&gt;Right so we have written out first file, let&amp;rsquo;s look at the other two simple functions supported by the Grid class.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var MongoClient = require(&#39;mongodb&#39;).MongoClient,
  Grid = mongo.Grid;

// Connect to the db
MongoClient.connect(&amp;quot;mongodb://localhost:27017/exampleDb&amp;quot;, function(err, db) {
  if(err) return console.dir(err);

  var grid = new Grid(db, &#39;fs&#39;);    
  var buffer = new Buffer(&amp;quot;Hello world&amp;quot;);
  grid.put.(buffer, {metadata:{category:&#39;text&#39;}, content_type: &#39;text&#39;}, function(err, fileInfo) {        
    grid.get(fileInfo._id, function(err, data) {
      console.log(&amp;quot;Retrieved data: &amp;quot; + data.toString());
      grid.delete(fileInfo._id, function(err, result) {
      });        
    });
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at the two operations &lt;strong&gt;get&lt;/strong&gt; and &lt;strong&gt;delete&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grid.get(fileInfo._id, function(err, data) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;get&lt;/strong&gt; method takes an ObjectID as the first argument and as we can se in the code we are using the one provided in &lt;strong&gt;fileInfo._id&lt;/strong&gt;. This will read all the chunks for the file and return it as a Buffer object.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;delete&lt;/strong&gt; method also takes an ObjectID as the first argument but will delete the file entry and the chunks associated with the file in Mongo.&lt;/p&gt;

&lt;p&gt;This &lt;strong&gt;api&lt;/strong&gt; is the simplest one you can use to interact with GridFS but it&amp;rsquo;s not suitable for all kinds of files. One of it&amp;rsquo;s main drawbacks is you are trying to write large files to Mongo. This api will require you to read the entire file into memory when writing and reading from Mongo which most likely is not feasible if you have to store large files like Video or RAW Pictures. Luckily this is not the only way to work with GridFS. That&amp;rsquo;s not to say this api is not useful. If you are storing tons of small files the memory usage vs the simplicity might be a worthwhile tradeoff. Let&amp;rsquo;s dive into some of the more advanced ways of using GridFS.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Advanced GridFS or how not to run out of memory&lt;/h2&gt;

&lt;p&gt;As we just said controlling memory consumption for you file writing and reading is key if you want to scale up the application. That means not reading in entire files before either writing or reading from Mongo DB. The good news is, it&amp;rsquo;s supported. Let&amp;rsquo;s throw some code out there straight away and look at how to do chunk sized streaming writes and reads.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the requires and and other initializing stuff omitted for brevity&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var fileId = new ObjectID();
var gridStore = new GridStore(db, fileId, &amp;quot;w&amp;quot;, {root:&#39;fs&#39;});
gridStore.chunkSize = 1024 * 256;

gridStore.open(function(err, gridStore) {
 Step(
   function writeData() {
     var group = this.group();

     for(var i = 0; i &amp;lt; 1000000; i += 5000) {
       gridStore.write(new Buffer(5000), group());
     }   
   },

   function doneWithWrite() {
     gridStore.close(function(err, result) {
       console.log(&amp;quot;File has been written to GridFS&amp;quot;);
     });
   }
 )
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we jump into picking apart the code let&amp;rsquo;s look at&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var gridStore = new GridStore(db, fileId, &amp;quot;w&amp;quot;, {root:&#39;fs&#39;});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the parameter &lt;strong&gt;&amp;ldquo;w&amp;rdquo;&lt;/strong&gt; this is important. It tells the driver that you are planning to write a new file. The parameters you can use here are.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;r&amp;rdquo;&lt;/strong&gt; - read only. This is the default mode&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;w&amp;rdquo;&lt;/strong&gt; - write in truncate mode. Existing data will be overwritten&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;ldquo;w+&amp;rdquo;&lt;/strong&gt; - write in edit mode&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Right so there is a fair bit to digest here. We are simulating writing a file that&amp;rsquo;s about 1MB big to  Mongo DB using GridFS. To do this we are writing it in chunks of 5000 bytes. So to not live with a difficult callback setup we are using the Step library with its&amp;rsquo; group functionality to ensure that we are notified when all of the writes are done. After all the writes are done Step will invoke the next function (or step) called &lt;strong&gt;doneWithWrite&lt;/strong&gt; where we finish up by closing the file that flushes out any remaining data to Mongo DB and updates the file document.&lt;/p&gt;

&lt;p&gt;As we are doing it in chunks of 5000 bytes we will notice that memory consumption is low. This is the trick to write large files to GridFS. In pieces. Also notice this line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.chunkSize = 1024 * 256;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This allows you to adjust how big the chunks are in bytes that Mongo DB will write. You can tune the Chunk Size to your needs. If you need to write large files to GridFS it might be worthwhile to trade of memory for CPU by setting a larger Chunk Size.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s see how the actual streaming read works.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;new GridStore(db, fileId, &amp;quot;r&amp;quot;).open(function(err, gridStore) {
  var stream = gridStore.stream(true);

  stream.on(&amp;quot;data&amp;quot;, function(chunk) {
    console.log(&amp;quot;Chunk of file data&amp;quot;);
  });

  stream.on(&amp;quot;end&amp;quot;, function() {
    console.log(&amp;quot;EOF of file&amp;quot;);
  });

  stream.on(&amp;quot;close&amp;quot;, function() {
    console.log(&amp;quot;Finished reading the file&amp;quot;);
  });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right let&amp;rsquo;s have a quick lock at the streaming functionality supplied with the driver &lt;strong&gt;(make sure you are using 0.9.6-12 or higher as there is a bug fix for custom chunksizes that you need)&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var stream = gridStore.stream(true);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This opens a stream to our file, you can pass in a boolean parameter to tell the driver to close the file automatically when it reaches the end. This will fire the &lt;strong&gt;close&lt;/strong&gt; event automatically. Otherwise you&amp;rsquo;ll have to handle cleanup when you receive the &lt;strong&gt;end&lt;/strong&gt; event. Let&amp;rsquo;s have a look at the events supported.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  stream.on(&amp;quot;data&amp;quot;, function(chunk) {
    console.log(&amp;quot;Chunk of file data&amp;quot;);
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;data&lt;/strong&gt; event is called for each chunk read. This means that it&amp;rsquo;s by the chunk size of the written file. So if you file is 1MB big and the file has chunkSize 256K then you&amp;rsquo;ll get 4 calls to the event handler for &lt;strong&gt;data&lt;/strong&gt;. The chunk returned is a &lt;strong&gt;Buffer&lt;/strong&gt; object.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  stream.on(&amp;quot;end&amp;quot;, function() {
    console.log(&amp;quot;EOF of file&amp;quot;);
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;end&lt;/strong&gt; event is called when the driver reaches the end of data for the file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  stream.on(&amp;quot;close&amp;quot;, function() {
    console.log(&amp;quot;Finished reading the file&amp;quot;);
  });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;close&lt;/strong&gt; event is only called if you the &lt;strong&gt;autoclose&lt;/strong&gt; parameter on the &lt;strong&gt;gridStore.stream&lt;/strong&gt; method as shown above. If it&amp;rsquo;s false or not set handle cleanup of the streaming in the &lt;strong&gt;end&lt;/strong&gt; event handler.&lt;/p&gt;

&lt;p&gt;Right that&amp;rsquo;s it for writing to GridFS in an efficient Manner. I&amp;rsquo;ll outline some other useful function on the Gridstore object.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Other useful methods on the Gridstore object&lt;/h2&gt;

&lt;p&gt;There are some other methods that are useful&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.writeFile(filename/filedescriptor, function(err fileInfo) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;writeFile&lt;/strong&gt; takes either a file name or a file descriptor and writes it to GridFS. It does this in chunks to ensure the Eventloop is not tied up.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.read(length, function(err, data) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;read/readBuffer&lt;/strong&gt; lets you read a #length number of bytes from the current position in the file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.seek(position, seekLocation, function(err, gridStore) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;seek&lt;/strong&gt; lets you navigate the file to read from different positions inside the chunks. The seekLocation allows you to specify how to seek. It can be one of three values.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GridStore.IO_SEEK_SET Seek mode where the given length is absolute&lt;/li&gt;
&lt;li&gt;GridStore.IO_SEEK_CUR Seek mode where the given length is an offset to the current read/write head&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GridStore.IO_SEEK_END Seek mode where the given length is an offset to the end of the file&lt;/p&gt;

&lt;p&gt;GridStore.list(dbInstance, collectionName, {id:true}, function(err, files) {})&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;list&lt;/strong&gt; lists all the files in the collection in GridFS. If you have a lot of files the current version will not work very well as it&amp;rsquo;s getting all files into memory first. You can have it return either the filenames or the ids for the files using option.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gridStore.unlink(function(err, result) {});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;unlink&lt;/strong&gt; deletes the file from Mongo DB, that&amp;rsquo;s to say all the file info and all the chunks.&lt;/p&gt;

&lt;p&gt;This should be plenty to get you on your way building your first GridFS based application. As in the previous article the following links might be useful for you. Good luck and have fun.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Links and stuff&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/examples&#34;&gt;The driver examples, good starting point for basic usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mongodb/node-mongodb-native/tree/master/test&#34;&gt;All the integration tests, they have tons of different usage cases&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
